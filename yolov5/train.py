# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Train a YOLOv5 model on a custom dataset

Usage:
    $ python path/to/train.py --data coco128.yaml --weights yolov5s.pt --img 640
"""

import argparse
import logging
import math
import os
import random
import sys
import time
from copy import deepcopy
from pathlib import Path

import numpy as np
import torch
import torch.distributed as dist
import torch.nn as nn
import yaml
from torch.cuda import amp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim import Adam, SGD, lr_scheduler
from tqdm import tqdm

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

import val  # for end-of-epoch mAP
from models.experimental import attempt_load
from models.yolo import Model
from utils.autoanchor import check_anchors
from utils.datasets import create_dataloader
from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \
    strip_optimizer, get_latest_run, check_dataset, check_git_status, check_img_size, check_requirements, \
    check_file, check_yaml, check_suffix, print_args, print_mutation, set_logging, one_cycle, colorstr, methods
from utils.downloads import attempt_download
from utils.loss import ComputeLoss
from utils.plots import plot_labels, plot_evolve
from utils.torch_utils import EarlyStopping, ModelEMA, de_parallel, intersect_dicts, select_device, \
    torch_distributed_zero_first
from utils.loggers.wandb.wandb_utils import check_wandb_resume
from utils.metrics import fitness
from utils.loggers import Loggers
from utils.callbacks import Callbacks

LOGGER = logging.getLogger(__name__)
LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html
RANK = int(os.getenv('RANK', -1))
WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))


def train(hyp,  # path/to/hyp.yaml æˆ–è€…è¶…å‚æ•°å­—å…¸
          opt,
          device,
          callbacks
          ):
    # è®¾ç½®è®­ç»ƒç›¸å…³çš„ç›®å½•å’Œå‚æ•°
    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze, = \
        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \
        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze

    # åˆ›å»ºä¿å­˜æ¨¡å‹æƒé‡çš„ç›®å½•
    w = save_dir / 'weights'  # æƒé‡ä¿å­˜ç›®å½•
    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # å¦‚æœéœ€è¦æ¼”åŒ–ï¼Œåˆ™åˆ›å»ºçˆ¶ç›®å½•ï¼Œå¦åˆ™åˆ›å»ºæƒé‡ç›®å½•
    last, best = w / 'last.pt', w / 'best.pt'  # å®šä¹‰æœ€åå’Œæœ€å¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„

    # åŠ è½½è¶…å‚æ•°
    if isinstance(hyp, str):
        with open(hyp, errors='ignore') as f:
            hyp = yaml.safe_load(f)  # ä» YAML æ–‡ä»¶ä¸­åŠ è½½è¶…å‚æ•°å­—å…¸
    LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))  # è®°å½•è¶…å‚æ•°ä¿¡æ¯

    # ä¿å­˜è¿è¡Œè®¾ç½®
    with open(save_dir / 'hyp.yaml', 'w') as f:
        yaml.safe_dump(hyp, f, sort_keys=False)  # ä¿å­˜è¶…å‚æ•°åˆ° YAML æ–‡ä»¶
    with open(save_dir / 'opt.yaml', 'w') as f:
        yaml.safe_dump(vars(opt), f, sort_keys=False)  # ä¿å­˜è®­ç»ƒé€‰é¡¹åˆ° YAML æ–‡ä»¶
    data_dict = None  # åˆå§‹åŒ–æ•°æ®å­—å…¸

    # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
    if RANK in [-1, 0]:  # ä»…åœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œ
        loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # åˆ›å»ºæ—¥å¿—è®°å½•å™¨å®ä¾‹
        if loggers.wandb:  # å¦‚æœä½¿ç”¨ wandb è¿›è¡Œå®éªŒè¿½è¸ª
            data_dict = loggers.wandb.data_dict  # è·å– wandb çš„æ•°æ®å­—å…¸
            if resume:  # å¦‚æœæ˜¯æ¢å¤è®­ç»ƒ
                weights, epochs, hyp = opt.weights, opt.epochs, opt.hyp  # æ›´æ–°æƒé‡å’Œè¶…å‚æ•°

        # æ³¨å†Œå›è°ƒå‡½æ•°
        for k in methods(loggers):  # éå†æ—¥å¿—è®°å½•å™¨çš„æ–¹æ³•
            callbacks.register_action(k, callback=getattr(loggers, k))  # å°†æ—¥å¿—è®°å½•å™¨çš„æ–¹æ³•æ³¨å†Œä¸ºå›è°ƒ

    # é…ç½®
    plots = not evolve  # æ˜¯å¦åˆ›å»ºç»˜å›¾ï¼Œæ¼”åŒ–æ¨¡å¼ä¸‹ä¸åˆ›å»º
    cuda = device.type != 'cpu'  # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ CUDAï¼ˆGPUï¼‰
    init_seeds(1 + RANK)  # åˆå§‹åŒ–éšæœºç§å­ï¼Œç¡®ä¿æ¯ä¸ªè¿›ç¨‹çš„ç§å­ä¸åŒ

    # åœ¨åˆ†å¸ƒå¼è®­ç»ƒçš„ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œä»¥ä¸‹æ“ä½œ
    with torch_distributed_zero_first(LOCAL_RANK):
        data_dict = data_dict or check_dataset(data)  # æ£€æŸ¥æ•°æ®é›†ï¼Œå¦‚æœæ•°æ®å­—å…¸ä¸º Noneï¼Œåˆ™åŠ è½½æ•°æ®é›†

    # è·å–è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†çš„è·¯å¾„
    train_path, val_path = data_dict['train'], data_dict['val']
    nc = 1 if single_cls else int(data_dict['nc'])  # è·å–ç±»åˆ«æ•°é‡ï¼Œå•ç±»åˆ«æƒ…å†µä¸‹æ•°é‡ä¸º 1
    # è·å–ç±»åˆ«åç§°ï¼Œå¦‚æœæ˜¯å•ç±»åˆ«ä¸”åç§°åˆ—è¡¨é•¿åº¦ä¸ä¸º 1ï¼Œåˆ™è®¾ä¸º ['item']
    names = ['item'] if single_cls and len(data_dict['names']) != 1 else data_dict['names']
    assert len(names) == nc, f'{len(names)} names found for nc={nc} dataset in {data}'  # æ£€æŸ¥ç±»åˆ«åç§°çš„æ•°é‡æ˜¯å¦ä¸ nc åŒ¹é…
    is_coco = data.endswith('coco.yaml') and nc == 80  # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸º COCO æ•°æ®é›†ï¼Œä¸”ç±»åˆ«æ•°é‡æ˜¯å¦ä¸º 80

    # Model
    check_suffix(weights, '.pt')  # æ£€æŸ¥æƒé‡æ–‡ä»¶çš„åç¼€æ˜¯å¦ä¸º .pt
    pretrained = weights.endswith('.pt')  # åˆ¤æ–­æƒé‡æ–‡ä»¶æ˜¯å¦ä¸ºé¢„è®­ç»ƒæ¨¡å‹

    if pretrained:
        # å¦‚æœæ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™å°è¯•ä¸‹è½½å®ƒ
        with torch_distributed_zero_first(LOCAL_RANK):
            weights = attempt_download(weights)  # å¦‚æœæœ¬åœ°æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶ï¼Œåˆ™ä¸‹è½½

        ckpt = torch.load(weights, map_location=device)  # åŠ è½½æ£€æŸ¥ç‚¹
        # åˆ›å»ºæ¨¡å‹ï¼Œcfg ä¸ºé…ç½®æ–‡ä»¶ï¼Œch ä¸ºè¾“å…¥é€šé“æ•°ï¼ˆä¸€èˆ¬ä¸º3ï¼‰ï¼Œnc ä¸ºç±»åˆ«æ•°ï¼Œanchors ä¸ºé”šæ¡†
        model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # åˆ›å»ºæ¨¡å‹å®ä¾‹

        exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # å®šä¹‰éœ€è¦æ’é™¤çš„é”®
        csd = ckpt['model'].float().state_dict()  # è·å–æ£€æŸ¥ç‚¹çš„ state_dictï¼Œè½¬ä¸º FP32 æ ¼å¼
        csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # äº¤é›†ï¼Œè·å–åŒ¹é…çš„å‚æ•°
        model.load_state_dict(csd, strict=False)  # åŠ è½½å‚æ•°
        LOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # è¾“å‡ºè½¬ç§»çš„å‚æ•°æ•°é‡
    else:
        # å¦‚æœä¸æ˜¯é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™ä½¿ç”¨ç»™å®šçš„ cfg åˆ›å»ºæ–°æ¨¡å‹
        model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # åˆ›å»ºæ¨¡å‹å®ä¾‹

    # Freeze
    freeze = [f'model.{x}.' for x in range(freeze)]  # å®šä¹‰éœ€è¦å†»ç»“çš„å±‚
    for k, v in model.named_parameters():
        v.requires_grad = True  # é»˜è®¤æ‰€æœ‰å±‚å‡å¯è®­ç»ƒ
        if any(x in k for x in freeze):  # æ£€æŸ¥å½“å‰å±‚æ˜¯å¦åœ¨å†»ç»“åˆ—è¡¨ä¸­
            print(f'freezing {k}')  # æ‰“å°å†»ç»“å±‚çš„ä¿¡æ¯
            v.requires_grad = False  # å†»ç»“è¯¥å±‚çš„å‚æ•°

    # ä¼˜åŒ–å™¨
    nbs = 64  # è§„å®šçš„æ‰¹é‡å¤§å°
    accumulate = max(round(nbs / batch_size), 1)  # åœ¨ä¼˜åŒ–ä¹‹å‰ç´¯ç§¯æŸå¤±
    hyp['weight_decay'] *= batch_size * accumulate / nbs  # æŒ‰ç…§æ‰¹é‡å¤§å°ç¼©æ”¾ weight_decay
    LOGGER.info(f"Scaled weight_decay = {hyp['weight_decay']}")  # è®°å½•ç¼©æ”¾åçš„ weight_decay

    g0, g1, g2 = [], [], []  # å®šä¹‰ä¼˜åŒ–å™¨å‚æ•°ç»„

    # éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—
    for v in model.modules():
        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):  # å¦‚æœæ¨¡å—æœ‰åç½®
            g2.append(v.bias)  # å°†åç½®æ·»åŠ åˆ° g2
        if isinstance(v, nn.BatchNorm2d):  # å¦‚æœæ¨¡å—æ˜¯ BatchNorm2d
            g0.append(v.weight)  # å°†æƒé‡æ·»åŠ åˆ° g0ï¼ˆä¸ä½¿ç”¨æƒé‡è¡°å‡ï¼‰
        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):  # å¦‚æœæ¨¡å—æœ‰æƒé‡
            g1.append(v.weight)  # å°†æƒé‡æ·»åŠ åˆ° g1ï¼ˆä½¿ç”¨æƒé‡è¡°å‡ï¼‰

    # æ ¹æ®é€‰æ‹©çš„ä¼˜åŒ–å™¨ç±»å‹åˆ›å»ºä¼˜åŒ–å™¨
    if opt.adam:
        # ä½¿ç”¨ Adam ä¼˜åŒ–å™¨ï¼Œè°ƒæ•´ beta1 ä¸ºåŠ¨é‡
        optimizer = Adam(g0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))
    else:
        # ä½¿ç”¨ SGD ä¼˜åŒ–å™¨
        optimizer = SGD(g0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)

    # æ·»åŠ å‚æ•°ç»„ g1ï¼ˆä½¿ç”¨ weight_decayï¼‰å’Œ g2ï¼ˆåç½®ï¼‰
    optimizer.add_param_group({'params': g1, 'weight_decay': hyp['weight_decay']})
    optimizer.add_param_group({'params': g2})  # æ·»åŠ åç½® g2
    LOGGER.info(f"{colorstr('optimizer:')} {type(optimizer).__name__} with parameter groups "
                f"{len(g0)} weight, {len(g1)} weight (no decay), {len(g2)} bias")  # è®°å½•ä¼˜åŒ–å™¨ä¿¡æ¯

    # æ¸…ç†ä¸å†éœ€è¦çš„å‚æ•°ç»„
    del g0, g1, g2

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if opt.linear_lr:
        # å¦‚æœé€‰æ‹©çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼Œå®šä¹‰å­¦ä¹ ç‡å‡½æ•° lf
        lf = lambda x: (1 - x / (epochs - 1)) * (1.0 - hyp['lrf']) + hyp['lrf']  # çº¿æ€§è°ƒåº¦
    else:
        # å¦åˆ™ä½¿ç”¨ä½™å¼¦è°ƒåº¦ï¼Œåˆ›å»ºå­¦ä¹ ç‡å‡½æ•° lf
        lf = one_cycle(1, hyp['lrf'], epochs)  # ä» 1 åˆ° hyp['lrf'] çš„ä½™å¼¦è°ƒåº¦

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå°†ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡å‡½æ•° lf ä¼ å…¥
    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)
    # å¯è§†åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨ (å¯é€‰)
    # plot_lr_scheduler(optimizer, scheduler, epochs)

    # EMA
    ema = ModelEMA(model) if RANK in [-1, 0] else None

    # Resume
    start_epoch, best_fitness = 0, 0.0  # åˆå§‹åŒ–å¼€å§‹çš„è½®æ¬¡å’Œæœ€ä½³é€‚åº”åº¦
    if pretrained:  # å¦‚æœä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
        # Optimizer
        if ckpt['optimizer'] is not None:  # å¦‚æœæ£€æŸ¥ç‚¹ä¸­åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€
            optimizer.load_state_dict(ckpt['optimizer'])  # åŠ è½½ä¼˜åŒ–å™¨çš„çŠ¶æ€å­—å…¸
            best_fitness = ckpt['best_fitness']  # æ›´æ–°æœ€ä½³é€‚åº”åº¦

        # EMA
        if ema and ckpt.get('ema'):  # å¦‚æœå¯ç”¨ EMA ä¸”æ£€æŸ¥ç‚¹ä¸­åŒ…å« EMA çŠ¶æ€
            ema.ema.load_state_dict(ckpt['ema'].float().state_dict())  # åŠ è½½ EMA çš„çŠ¶æ€å­—å…¸
            ema.updates = ckpt['updates']  # æ›´æ–° EMA çš„æ¬¡æ•°

        # Epochs
        start_epoch = ckpt['epoch'] + 1  # è®¾ç½®å¼€å§‹çš„è½®æ¬¡ä¸ºæ£€æŸ¥ç‚¹çš„è½®æ¬¡åŠ  1
        if resume:  # å¦‚æœé€‰æ‹©äº†æ¢å¤è®­ç»ƒ
            assert start_epoch > 0, f'{weights} training to {epochs} epochs is finished, nothing to resume.'  # ç¡®ä¿å¯ä»¥æ¢å¤
        if epochs < start_epoch:  # å¦‚æœè®¾ç½®çš„è½®æ¬¡å°äºæ¢å¤çš„è½®æ¬¡
            LOGGER.info(
                f"{weights} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {epochs} more epochs.")  # æ—¥å¿—è®°å½•
            epochs += ckpt['epoch']  # ç»§ç»­è®­ç»ƒæ›´å¤šè½®æ¬¡

        del ckpt, csd  # æ¸…ç†æ£€æŸ¥ç‚¹å’Œå…¶ä»–å˜é‡ä»¥é‡Šæ”¾å†…å­˜

    # Image sizes
    gs = max(int(model.stride.max()), 32)  # è·å–æ¨¡å‹çš„æœ€å¤§æ­¥å¹…ä½œä¸ºç½‘æ ¼å¤§å°ï¼Œç¡®ä¿è‡³å°‘ä¸º 32
    nl = model.model[-1].nl  # è·å–æ£€æµ‹å±‚çš„æ•°é‡ï¼ˆç”¨äºç¼©æ”¾ hyp['obj'] è¶…å‚æ•°ï¼‰
    imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # éªŒè¯å›¾åƒå¤§å°æ˜¯å¦æ˜¯ç½‘æ ¼å¤§å°çš„å€æ•°ï¼Œä¸”ä¸å°äº gs çš„ä¸¤å€

    # DP mode
    if cuda and RANK == -1 and torch.cuda.device_count() > 1:
        # å¦‚æœåœ¨å¤š GPU ç¯å¢ƒä¸‹ä¸”æœªä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œå‘å‡ºè­¦å‘Š
        logging.warning('DP not recommended, instead use torch.distributed.run for best DDP Multi-GPU results.\n'
                        'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')
        model = torch.nn.DataParallel(model)  # ä½¿ç”¨æ•°æ®å¹¶è¡Œï¼ˆDataParallelï¼‰

    # SyncBatchNorm
    if opt.sync_bn and cuda and RANK != -1:
        # å¦‚æœå¯ç”¨äº†åŒæ­¥æ‰¹å½’ä¸€åŒ–ä¸”å¤„äºåˆ†å¸ƒå¼è®­ç»ƒæ¨¡å¼ï¼Œè½¬æ¢æ¨¡å‹ä¸ºåŒæ­¥æ‰¹å½’ä¸€åŒ–
        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)
        LOGGER.info('Using SyncBatchNorm()')  # è®°å½•ä½¿ç”¨åŒæ­¥æ‰¹å½’ä¸€åŒ–çš„ä¿¡æ¯

    # Trainloader
    train_loader, dataset = create_dataloader(
        train_path, imgsz, batch_size // WORLD_SIZE, gs, single_cls,
        hyp=hyp, augment=True, cache=opt.cache, rect=opt.rect,
        rank=LOCAL_RANK, workers=workers, image_weights=opt.image_weights,
        quad=opt.quad, prefix=colorstr('train: ')
    )  # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨å’Œæ•°æ®é›†

    mlc = int(np.concatenate(dataset.labels, 0)[:, 0].max())  # æ‰¾åˆ°æ•°æ®é›†ä¸­æœ€å¤§æ ‡ç­¾ç±»
    nb = len(train_loader)  # è®¡ç®—æ‰¹æ¬¡çš„æ•°é‡

    # æ£€æŸ¥æœ€å¤§æ ‡ç­¾ç±»æ˜¯å¦å°äºç±»åˆ«æ€»æ•°
    assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'

    # å¤„ç†è¿‡ç¨‹ 0
    if RANK in [-1, 0]:
        # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨ï¼Œæ‰¹å¤§å°æ˜¯åŸæ¥çš„ä¸¤å€
        val_loader = create_dataloader(
            val_path, imgsz, batch_size // WORLD_SIZE * 2, gs, single_cls,
            hyp=hyp, cache=None if noval else opt.cache, rect=True, rank=-1,
            workers=workers, pad=0.5,
            prefix=colorstr('val: ')
        )[0]

        # å¦‚æœä¸æ˜¯æ¢å¤è®­ç»ƒ
        if not resume:
            labels = np.concatenate(dataset.labels, 0)  # åˆå¹¶æ‰€æœ‰æ ‡ç­¾
            # c = torch.tensor(labels[:, 0])  # æå–ç±»åˆ«
            # cf = torch.bincount(c.long(), minlength=nc) + 1.  # ç»Ÿè®¡é¢‘ç‡
            # model._initialize_biases(cf.to(device))  # åˆå§‹åŒ–åç½®

            if plots:
                plot_labels(labels, names, save_dir)  # ç»˜åˆ¶æ ‡ç­¾åˆ†å¸ƒ

            # é”šæ¡†æ£€æŸ¥
            if not opt.noautoanchor:
                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)

            model.half().float()  # é¢„å…ˆå‡å°‘é”šæ¡†ç²¾åº¦

        callbacks.run('on_pretrain_routine_end')  # è¿è¡Œè®­ç»ƒå‰ä¾‹ç¨‹ç»“æŸçš„å›è°ƒ

    # DDPæ¨¡å¼
    if cuda and RANK != -1:
        # ä½¿ç”¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰åŒ…è£…æ¨¡å‹
        model = DDP(model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK)

    # æ¨¡å‹å‚æ•°
    hyp['box'] *= 3. / nl  # å°†æ¡†çš„è¶…å‚æ•°ç¼©æ”¾åˆ°æ£€æµ‹å±‚æ•°é‡
    hyp['cls'] *= nc / 80. * 3. / nl  # å°†ç±»åˆ«è¶…å‚æ•°ç¼©æ”¾åˆ°ç±»åˆ«æ•°é‡å’Œæ£€æµ‹å±‚æ•°é‡
    hyp['obj'] *= (imgsz / 640) ** 2 * 3. / nl  # å°†ç›®æ ‡è¶…å‚æ•°ç¼©æ”¾åˆ°å›¾åƒå°ºå¯¸å’Œæ£€æµ‹å±‚æ•°é‡
    hyp['label_smoothing'] = opt.label_smoothing  # è®¾ç½®æ ‡ç­¾å¹³æ»‘å‚æ•°
    # å°†ç±»åˆ«æ•°é‡é™„åŠ åˆ°æ¨¡å‹
    model.nc = nc  # attach number of classes to model
    # å°†è¶…å‚æ•°é™„åŠ åˆ°æ¨¡å‹
    model.hyp = hyp  # attach hyperparameters to model
    # è®¡ç®—å¹¶é™„åŠ ç±»åˆ«æƒé‡åˆ°æ¨¡å‹
    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights
    # å°†ç±»åˆ«åç§°é™„åŠ åˆ°æ¨¡å‹
    model.names = names  # attach class names to model

    # å¼€å§‹è®­ç»ƒ
    t0 = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # è®¡ç®—é¢„çƒ­è¿­ä»£æ¬¡æ•°ï¼Œæœ€å°ä¸º1000æ¬¡ï¼ˆç›¸å½“äº3ä¸ªepochï¼‰
    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # é™åˆ¶é¢„çƒ­æ—¶é—´å°äºæ€»è®­ç»ƒæ—¶é—´çš„ä¸€åŠ

    last_opt_step = -1  # æœ€åä¸€æ¬¡ä¼˜åŒ–æ­¥éª¤
    maps = np.zeros(nc)  # æ¯ä¸ªç±»åˆ«çš„mAP
    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)

    scheduler.last_epoch = start_epoch - 1  # è®¾ç½®è°ƒåº¦å™¨çš„æœ€åepochä¸ºå½“å‰epochä¹‹å‰
    scaler = amp.GradScaler(enabled=cuda)  # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾å™¨
    stopper = EarlyStopping(patience=opt.patience)  # åˆå§‹åŒ–æ—©åœæœºåˆ¶ï¼Œè®¾å®šè€å¿ƒå€¼
    compute_loss = ComputeLoss(model)  # åˆå§‹åŒ–æŸå¤±è®¡ç®—ç±»

    # è®°å½•è®­ç»ƒä¿¡æ¯
    LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\n'
                f'Using {train_loader.num_workers} dataloader workers\n'
                f"Logging results to {colorstr('bold', save_dir)}\n"
                f'Starting training for {epochs} epochs...')  # æ‰“å°è®­ç»ƒå¼€å§‹ä¿¡æ¯

    for epoch in range(start_epoch, epochs):  # è®­ç»ƒå‘¨æœŸå¾ªç¯
        model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼

        # å¯é€‰ï¼šæ›´æ–°å›¾åƒæƒé‡ï¼ˆä»…é€‚ç”¨äºå•GPUï¼‰
        if opt.image_weights:
            cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # è®¡ç®—ç±»åˆ«æƒé‡
            iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # è®¡ç®—å›¾åƒæƒé‡
            dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # éšæœºåŠ æƒç´¢å¼•

        # å¯é€‰ï¼šæ›´æ–°é©¬èµ›å…‹è¾¹æ¡†
        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)
        # dataset.mosaic_border = [b - imgsz, -b]  # è®¾ç½®é«˜åº¦å’Œå®½åº¦è¾¹æ¡†

        mloss = torch.zeros(3, device=device)  # åˆå§‹åŒ–å¹³å‡æŸå¤±
        if RANK != -1:
            train_loader.sampler.set_epoch(epoch)  # è®¾ç½®è®­ç»ƒåŠ è½½å™¨çš„å½“å‰å‘¨æœŸ
        pbar = enumerate(train_loader)  # éå†è®­ç»ƒæ•°æ®åŠ è½½å™¨
        LOGGER.info(('\n' + '%10s' * 7) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'labels', 'img_size'))  # æ—¥å¿—ä¿¡æ¯
        if RANK in [-1, 0]:
            pbar = tqdm(pbar, total=nb)  # æ˜¾ç¤ºè¿›åº¦æ¡
        optimizer.zero_grad()  # ä¼˜åŒ–å™¨æ¢¯åº¦æ¸…é›¶
        for i, (imgs, targets, paths, _) in pbar:  # æ‰¹å¤„ç†å¾ªç¯
            ni = i + nb * epoch  # è®¡ç®—è‡ªè®­ç»ƒå¼€å§‹ä»¥æ¥çš„é›†æˆæ‰¹æ¬¡æ•°
            imgs = imgs.to(device, non_blocking=True).float() / 255.0  # å°†uint8ç±»å‹è½¬æ¢ä¸ºfloat32å¹¶å½’ä¸€åŒ–

            # Warmupé˜¶æ®µ
            if ni <= nw:
                xi = [0, nw]  # çº¿æ€§æ’å€¼èŒƒå›´
                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # IOUæŸå¤±æ¯”ç‡ï¼ˆobj_loss = 1.0æˆ–IOUï¼‰
                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())  # è®¡ç®—ç´¯ç§¯æ­¥éª¤
                for j, x in enumerate(optimizer.param_groups):  # éå†ä¼˜åŒ–å™¨å‚æ•°ç»„
                    # æ›´æ–°å­¦ä¹ ç‡ï¼šåç½®å­¦ä¹ ç‡ä»0.1é™åˆ°lr0ï¼Œå…¶ä»–å­¦ä¹ ç‡ä»0.0å‡åˆ°lr0
                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])
                    if 'momentum' in x:
                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])  # æ›´æ–°åŠ¨é‡

            # å¤šå°ºåº¦è®­ç»ƒ
            if opt.multi_scale:
                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # éšæœºé€‰æ‹©å¤§å°
                sf = sz / max(imgs.shape[2:])  # è®¡ç®—ç¼©æ”¾å› å­
                if sf != 1:
                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # è®¡ç®—æ–°å½¢çŠ¶ï¼ˆè°ƒæ•´ä¸ºgsçš„å€æ•°ï¼‰
                    imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)  # é‡æ–°è°ƒæ•´å›¾åƒå¤§å°

            # å‰å‘ä¼ æ’­
            with amp.autocast(enabled=cuda):  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
                pred = model(imgs)  # å‰å‘ä¼ æ’­å¾—åˆ°é¢„æµ‹
                loss, loss_items = compute_loss(pred, targets.to(device))  # è®¡ç®—æŸå¤±
                if RANK != -1:
                    loss *= WORLD_SIZE  # åœ¨DDPæ¨¡å¼ä¸‹è¿›è¡Œæ¢¯åº¦å¹³å‡
                if opt.quad:
                    loss *= 4.  # å¦‚æœä½¿ç”¨å››å…ƒç»„ï¼Œåˆ™æŸå¤±ä¹˜ä»¥4

            # åå‘ä¼ æ’­
            scaler.scale(loss).backward()  # åå‘ä¼ æ’­å¹¶ç¼©æ”¾

            # ä¼˜åŒ–
            if ni - last_opt_step >= accumulate:  # å¦‚æœè¾¾åˆ°ç´¯ç§¯æ­¥éª¤
                scaler.step(optimizer)  # æ›´æ–°ä¼˜åŒ–å™¨
                scaler.update()  # æ›´æ–°ç¼©æ”¾å™¨
                optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
                if ema:  # å¦‚æœä½¿ç”¨EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰
                    ema.update(model)  # æ›´æ–°EMA
                last_opt_step = ni  # æ›´æ–°ä¸Šä¸€æ¬¡ä¼˜åŒ–æ­¥éª¤

            # æ—¥å¿—è®°å½•
            if RANK in [-1, 0]:  # å¦‚æœæ˜¯ä¸»è¿›ç¨‹
                mloss = (mloss * i + loss_items) / (i + 1)  # æ›´æ–°å¹³å‡æŸå¤±
                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                pbar.set_description(('%10s' * 2 + '%10.4g' * 5) % (
                    f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1]))  # æ›´æ–°è¿›åº¦æ¡æè¿°
                callbacks.run('on_train_batch_end', ni, model, imgs, targets, paths, plots, opt.sync_bn)  # è°ƒç”¨å›è°ƒå‡½æ•°
        # ç»“æŸæ‰¹å¤„ç†å¾ªç¯

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        lr = [x['lr'] for x in optimizer.param_groups]  # è®°å½•å½“å‰å­¦ä¹ ç‡
        scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡

        if RANK in [-1, 0]:  # å¦‚æœæ˜¯ä¸»è¿›ç¨‹
            # è®¡ç®—mAPï¼ˆå¹³å‡ç²¾åº¦å‡å€¼ï¼‰
            callbacks.run('on_train_epoch_end', epoch=epoch)  # è¿è¡Œå›è°ƒå‡½æ•°ï¼Œè®°å½•è®­ç»ƒå‘¨æœŸç»“æŸ
            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])  # æ›´æ–°EMAæ¨¡å‹å±æ€§
            final_epoch = (epoch + 1 == epochs) or stopper.possible_stop  # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªå‘¨æœŸ

            if not noval or final_epoch:  # å¦‚æœä¸è¿›è¡ŒéªŒè¯æˆ–æ˜¯æœ€åä¸€ä¸ªå‘¨æœŸ
                results, maps, _ = val.run(data_dict,  # éªŒè¯æ¨¡å‹
                                           batch_size=batch_size // WORLD_SIZE * 2,  # è®¾ç½®éªŒè¯æ‰¹æ¬¡å¤§å°
                                           imgsz=imgsz,  # å›¾åƒå°ºå¯¸
                                           model=ema.ema,  # ä½¿ç”¨EMAæ¨¡å‹è¿›è¡ŒéªŒè¯
                                           single_cls=single_cls,  # æ˜¯å¦ä¸ºå•ç±»åˆ«
                                           dataloader=val_loader,  # éªŒè¯æ•°æ®åŠ è½½å™¨
                                           save_dir=save_dir,  # ä¿å­˜è·¯å¾„
                                           plots=False,  # æ˜¯å¦ç»˜åˆ¶å›¾
                                           callbacks=callbacks,  # å›è°ƒå‡½æ•°
                                           compute_loss=compute_loss)  # è®¡ç®—æŸå¤±

            # æ›´æ–°æœ€ä½³mAP
            fi = fitness(np.array(results).reshape(1, -1))  # è®¡ç®—é€‚åº”åº¦ï¼ˆåŠ æƒç»„åˆ[ç²¾åº¦, å¬å›ç‡, mAP@.5, mAP@.5-.95]
            if fi > best_fitness:  # å¦‚æœå½“å‰é€‚åº”åº¦å¤§äºæœ€ä½³é€‚åº”åº¦
                best_fitness = fi  # æ›´æ–°æœ€ä½³é€‚åº”åº¦

            log_vals = list(mloss) + list(results) + lr  # åˆå¹¶æŸå¤±ã€ç»“æœå’Œå­¦ä¹ ç‡
            callbacks.run('on_fit_epoch_end', log_vals, epoch, best_fitness, fi)  # è¿è¡Œé€‚åº”åº¦è®°å½•å›è°ƒ

            # ä¿å­˜æ¨¡å‹
            if (not nosave) or (final_epoch and not evolve):  # å¦‚æœéœ€è¦ä¿å­˜æ¨¡å‹
                ckpt = {'epoch': epoch,  # å½“å‰å‘¨æœŸ
                        'best_fitness': best_fitness,  # æœ€ä½³é€‚åº”åº¦
                        'model': deepcopy(de_parallel(model)).half(),  # æ·±æ‹·è´æ¨¡å‹å¹¶è½¬æ¢ä¸ºåŠç²¾åº¦
                        'ema': deepcopy(ema.ema).half(),  # æ·±æ‹·è´EMAæ¨¡å‹å¹¶è½¬æ¢ä¸ºåŠç²¾åº¦
                        'updates': ema.updates,  # EMAæ›´æ–°æ¬¡æ•°
                        'optimizer': optimizer.state_dict(),  # ä¼˜åŒ–å™¨çŠ¶æ€
                        'wandb_id': loggers.wandb.wandb_run.id if loggers.wandb else None}  # wandb ID

                # ä¿å­˜æœ€åæ¨¡å‹å’Œæœ€ä½³æ¨¡å‹ï¼Œå¹¶æ ¹æ®å‘¨æœŸåˆ é™¤
                torch.save(ckpt, last)  # ä¿å­˜æœ€åçš„æ¨¡å‹
                if best_fitness == fi:  # å¦‚æœå½“å‰é€‚åº”åº¦ä¸ºæœ€ä½³
                    torch.save(ckpt, best)  # ä¿å­˜æœ€ä½³æ¨¡å‹
                if (epoch > 0) and (opt.save_period > 0) and (epoch % opt.save_period == 0):  # æ ¹æ®å‘¨æœŸä¿å­˜æ¨¡å‹
                    torch.save(ckpt, w / f'epoch{epoch}.pt')  # ä¿å­˜æŒ‡å®šå‘¨æœŸçš„æ¨¡å‹
                del ckpt  # åˆ é™¤æ£€æŸ¥ç‚¹ä»¥é‡Šæ”¾å†…å­˜
                callbacks.run('on_model_save', last, epoch, final_epoch, best_fitness, fi)  # è¿è¡Œæ¨¡å‹ä¿å­˜å›è°ƒ

            # åœæ­¢å•GPUè®­ç»ƒ
            if RANK == -1 and stopper(epoch=epoch, fitness=fi):  # å¦‚æœæ˜¯å•GPUä¸”æ»¡è¶³åœæ­¢æ¡ä»¶
                break  # ç»“æŸè®­ç»ƒ
        # end epoch ----------------------------------------------------------------------------------------------------
    # end training -----------------------------------------------------------------------------------------------------
    if RANK in [-1, 0]:  # å¦‚æœæ˜¯ä¸»è¿›ç¨‹
        # è®°å½•å·²å®Œæˆçš„å‘¨æœŸå’Œè€—æ—¶
        LOGGER.info(f'\n{epoch - start_epoch + 1} epochs completed in {(time.time() - t0) / 3600:.3f} hours.')

        # å¯¹äºæœ€åä¸€ä¸ªå’Œæœ€ä½³æ¨¡å‹è¿›è¡Œå¤„ç†
        for f in last, best:
            if f.exists():  # å¦‚æœæ–‡ä»¶å­˜åœ¨
                strip_optimizer(f)  # å»é™¤ä¼˜åŒ–å™¨çŠ¶æ€ä»¥å‡å°æ¨¡å‹æ–‡ä»¶å¤§å°

                if f is best:  # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹
                    LOGGER.info(f'\nValidating {f}...')  # è®°å½•éªŒè¯ä¿¡æ¯
                    results, _, _ = val.run(data_dict,  # éªŒè¯æ¨¡å‹
                                            batch_size=batch_size // WORLD_SIZE * 2,  # è®¾ç½®æ‰¹æ¬¡å¤§å°
                                            imgsz=imgsz,  # å›¾åƒå°ºå¯¸
                                            model=attempt_load(f, device).half(),  # åŠ è½½æ¨¡å‹å¹¶è½¬æ¢ä¸ºåŠç²¾åº¦
                                            iou_thres=0.65 if is_coco else 0.60,  # è®¾ç½®IOUé˜ˆå€¼ï¼ˆé’ˆå¯¹COCOæ•°æ®é›†ï¼‰
                                            single_cls=single_cls,  # æ˜¯å¦ä¸ºå•ç±»åˆ«
                                            dataloader=val_loader,  # éªŒè¯æ•°æ®åŠ è½½å™¨
                                            save_dir=save_dir,  # ä¿å­˜è·¯å¾„
                                            save_json=is_coco,  # æ˜¯å¦ä¿å­˜ä¸ºJSONï¼ˆé’ˆå¯¹COCOæ•°æ®é›†ï¼‰
                                            verbose=True,  # æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
                                            plots=True,  # æ˜¯å¦ç»˜åˆ¶å›¾
                                            callbacks=callbacks,  # å›è°ƒå‡½æ•°
                                            compute_loss=compute_loss)  # è®¡ç®—æŸå¤±

        callbacks.run('on_train_end', last, best, plots, epoch)  # è¿è¡Œè®­ç»ƒç»“æŸçš„å›è°ƒ
        LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}")  # è®°å½•ç»“æœä¿å­˜è·¯å¾„

    torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
    return results  # è¿”å›ç»“æœ


def parse_opt(known=False):
    """
            å‡½æ•°åŠŸèƒ½ï¼šè®¾ç½®optå‚æ•°
    """
    parser = argparse.ArgumentParser()
    # --------------------------------------------------- å¸¸ç”¨å‚æ•° ---------------------------------------------
    parser.add_argument('--weights', type=str, default=ROOT / 'weights/yolov5s.pt', help='initial weights path')  # weights: æƒé‡æ–‡ä»¶
    parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='model.yaml path')  # cfg: ç½‘ç»œæ¨¡å‹é…ç½®æ–‡ä»¶ åŒ…æ‹¬ncã€depth_multipleã€width_multipleã€anchorsã€backboneã€headç­‰
    parser.add_argument('--data', type=str, default=ROOT / 'data/VOC-hat.yaml', help='dataset.yaml path')  # data: å®ç°æ•°æ®é›†é…ç½®æ–‡ä»¶ åŒ…æ‹¬pathã€trainã€valã€testã€ncã€namesç­‰
    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch.yaml', help='hyperparameters path')  # hyp: è®­ç»ƒæ—¶çš„è¶…å‚æ–‡ä»¶
    parser.add_argument('--epochs', type=int, default=100)  # epochs: è®­ç»ƒè½®æ¬¡
    parser.add_argument('--batch-size', type=int, default=64, help='total batch size for all GPUs')  # batch-size: è®­ç»ƒæ‰¹æ¬¡å¤§å°
    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=608, help='train, val image size (pixels)')  # imgsz: è¾“å…¥ç½‘ç»œçš„å›¾ç‰‡åˆ†è¾¨ç‡å¤§å°
    parser.add_argument('--rect', action='store_true', help='rectangular training')  # rect: æ˜¯å¦é‡‡ç”¨Rectangular training/inferenceï¼Œä¸€å¼ å›¾ç‰‡ä¸ºé•¿æ–¹å½¢ï¼Œæˆ‘ä»¬åœ¨å°†å…¶é€å…¥æ¨¡å‹å‰éœ€è¦å°†å…¶resizeåˆ°è¦æ±‚çš„å°ºå¯¸ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦é€šè¿‡è¡¥ç°paddingæ¥å˜ä¸ºæ­£æ–¹å½¢çš„å›¾ã€‚
    parser.add_argument('--resume', nargs='?', const=True, default="", help='resume most recent training')  # resume: æ–­ç‚¹ç»­è®­, ä»ä¸Šæ¬¡æ‰“æ–­çš„è®­ç»ƒç»“æœå¤„æ¥ç€è®­ç»ƒ  é»˜è®¤False
    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')  # nosave: ä¸ä¿å­˜æ¨¡å‹  é»˜è®¤ä¿å­˜  store_true: only test final epoch
    parser.add_argument('--noval', action='store_true', help='only validate final epoch')   # noval: åªåœ¨æœ€åä¸€æ¬¡è¿›è¡Œæµ‹è¯•ï¼Œé»˜è®¤False
    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')   # noautoanchor: ä¸è‡ªåŠ¨è°ƒæ•´anchor é»˜è®¤False(è‡ªåŠ¨è°ƒæ•´anchor)
    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')  # evolve: æ˜¯å¦è¿›è¡Œè¶…å‚è¿›åŒ–ï¼Œä½¿å¾—æ•°å€¼æ›´å¥½ é»˜è®¤False
    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')   # bucket: è°·æ­Œäº‘ç›˜bucket ä¸€èˆ¬ç”¨ä¸åˆ°
    parser.add_argument('--cache', type=str, nargs='?', const='ram', default="True", help='--cache images in "ram" (default) or "disk"')  # cache:æ˜¯å¦æå‰ç¼“å­˜å›¾ç‰‡åˆ°å†…å­˜ï¼Œä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦
    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')  #  image-weights: å¯¹äºé‚£äº›è®­ç»ƒä¸å¥½çš„å›¾ç‰‡ï¼Œä¼šåœ¨ä¸‹ä¸€è½®ä¸­å¢åŠ ä¸€äº›æƒé‡
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')  # device: è®­ç»ƒçš„è®¾å¤‡
    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')  # multi-scale: æ˜¯å¦ä½¿ç”¨å¤šå°ºåº¦è®­ç»ƒ é»˜è®¤Falseï¼Œè¦è¢«32æ•´é™¤ã€‚
    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')  # single-cls: æ•°æ®é›†æ˜¯å¦åªæœ‰ä¸€ä¸ªç±»åˆ« é»˜è®¤False
    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')  # adam: æ˜¯å¦ä½¿ç”¨adamä¼˜åŒ–å™¨
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')  # sync-bn: æ˜¯å¦ä½¿ç”¨è·¨å¡åŒæ­¥bnæ“ä½œ,å†DDPä¸­ä½¿ç”¨  é»˜è®¤False
    parser.add_argument('--workers', type=int, default=0, help='maximum number of dataloader workers')  # workers: dataloaderä¸­çš„æœ€å¤§workæ•°ï¼ˆçº¿ç¨‹ä¸ªæ•°ï¼‰
    parser.add_argument('--project', default=ROOT / 'runs/train', help='save to project/name')  # project: è®­ç»ƒç»“æœä¿å­˜çš„æ ¹ç›®å½• é»˜è®¤æ˜¯runs/train
    parser.add_argument('--name', default='exp', help='save to project/name')  # name: è®­ç»ƒç»“æœä¿å­˜çš„ç›®å½• é»˜è®¤æ˜¯exp
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')  # exist_ok: æ˜¯å¦é‡æ–°åˆ›å»ºæ—¥å¿—æ–‡ä»¶, Falseæ—¶é‡æ–°åˆ›å»ºæ–‡ä»¶(é»˜è®¤æ–‡ä»¶éƒ½æ˜¯ä¸å­˜åœ¨çš„)
    parser.add_argument('--quad', action='store_true', help='quad dataloader')  # quad: dataloaderå–æ•°æ®æ—¶, æ˜¯å¦ä½¿ç”¨collate_fn4ä»£æ›¿collate_fn  é»˜è®¤False
    parser.add_argument('--linear-lr', action='store_true', help='linear LR')  # linear-lrï¼šç”¨äºå¯¹å­¦ä¹ é€Ÿç‡è¿›è¡Œè°ƒæ•´ï¼Œé»˜è®¤ä¸º Falseï¼Œï¼ˆé€šè¿‡ä½™å¼¦å‡½æ•°æ¥é™ä½å­¦ä¹ ç‡ï¼‰
    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')  # label-smoothing: æ ‡ç­¾å¹³æ»‘å¢å¼º é»˜è®¤0.0ä¸å¢å¼º  è¦å¢å¼ºä¸€èˆ¬å°±è®¾ä¸º0.1
    parser.add_argument('--patience', type=int, default=100, help='EarlyStopping patience (epochs without improvement)')  # æ—©åœæœºåˆ¶ï¼Œè®­ç»ƒåˆ°ä¸€å®šçš„epochï¼Œå¦‚æœæ¨¡å‹æ•ˆæœæœªæå‡ï¼Œå°±è®©æ¨¡å‹æå‰åœæ­¢è®­ç»ƒã€‚
    parser.add_argument('--freeze', type=int, default=0, help='Number of layers to freeze. backbone=10, all=24')  # freeze: ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è§„å®šå›ºå®šæƒé‡ä¸è¿›è¡Œè°ƒæ•´  --freeze 10  :æ„æ€ä»ç¬¬0å±‚åˆ°åˆ°ç¬¬10å±‚ä¸è®­ç»ƒ
    parser.add_argument('--save-period', type=int, default=-1, help='Save checkpoint every x epochs (disabled if < 1)')  # è®¾ç½®å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')  # local_rank: rankä¸ºè¿›ç¨‹ç¼–å·  -1ä¸”gpu=1æ—¶ä¸è¿›è¡Œåˆ†å¸ƒå¼  -1ä¸”å¤šå—gpuä½¿ç”¨DataParallelæ¨¡å¼

    # --------------------------------------------------- W&B(wandb)å‚æ•° ---------------------------------------------
    parser.add_argument('--entity', default=None, help='W&B: Entity')  #wandb entity é»˜è®¤None
    parser.add_argument('--upload_dataset', action='store_true', help='W&B: Upload dataset as artifact table')  # æ˜¯å¦ä¸Šä¼ datasetåˆ°wandb tabel(å°†æ•°æ®é›†ä½œä¸ºäº¤äº’å¼ dsvizè¡¨ åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ã€æŸ¥è¯¢ã€ç­›é€‰å’Œåˆ†ææ•°æ®é›†) é»˜è®¤False
    parser.add_argument('--bbox_interval', type=int, default=-1, help='W&B: Set bounding-box image logging interval')  # è®¾ç½®ç•Œæ¡†å›¾åƒè®°å½•é—´éš” Set bounding-box image logging interval for W&B é»˜è®¤-1   opt.epochs // 10
    parser.add_argument('--artifact_alias', type=str, default='latest', help='W&B: Version of dataset artifact to use')

    opt = parser.parse_known_args()[0] if known else parser.parse_args()
    return opt


def main(opt, callbacks=Callbacks()):
    # è®¾ç½®æ—¥å¿—è®°å½•
    set_logging(RANK)

    # ä¸»è¿›ç¨‹æ£€æŸ¥
    if RANK in [-1, 0]:
        print_args(FILE.stem, opt)  # æ‰“å°è¿è¡Œå‚æ•°
        check_git_status()  # æ£€æŸ¥Gitä»“åº“çŠ¶æ€ï¼ˆç¡®ä¿ä»£ç æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼‰
        check_requirements(exclude=['thop'])  # æ£€æŸ¥ä¾èµ–åŒ…ï¼Œæ’é™¤'thop'åŒ…

    # æ¢å¤ä¸­æ–­çš„è¿è¡Œ
    if opt.resume and not check_wandb_resume(opt) and not opt.evolve:  # æ£€æŸ¥æ˜¯å¦ä»ä¸­æ–­ä½ç½®æ¢å¤
        ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # è·å–æŒ‡å®šæˆ–æœ€è¿‘çš„æ£€æŸ¥ç‚¹è·¯å¾„
        assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'  # æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        # ä»æŒ‡å®šæ£€æŸ¥ç‚¹ç›®å½•åŠ è½½è®­ç»ƒé…ç½®
        with open(Path(ckpt).parent.parent / 'opt.yaml', errors='ignore') as f:
            opt = argparse.Namespace(**yaml.safe_load(f))  # åŠ è½½è®­ç»ƒé…ç½®åˆ°optå˜é‡
        opt.cfg, opt.weights, opt.resume = '', ckpt, True  # è®¾ç½®é…ç½®æ–‡ä»¶è·¯å¾„å’Œæƒé‡æ–‡ä»¶è·¯å¾„
        LOGGER.info(f'Resuming training from {ckpt}')  # æ‰“å°æ¢å¤ä¿¡æ¯
    else:
        # æ ¡éªŒæ–‡ä»¶å’Œè·¯å¾„çš„é…ç½®
        opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = \
            check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(
                opt.project)  # æ£€æŸ¥é…ç½®æ–‡ä»¶è·¯å¾„
        assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'  # ç¡®ä¿cfgæˆ–weightså‚æ•°è‡³å°‘ä¸€ä¸ªå­˜åœ¨
        if opt.evolve:  # æ¼”åŒ–æ¨¡å¼
            opt.project = str(ROOT / 'runs/evolve')  # è®¾ç½®æ¼”åŒ–è¿è¡Œä¿å­˜è·¯å¾„
            opt.exist_ok, opt.resume = opt.resume, False  # è®¾ç½®è·¯å¾„æ˜¯å¦è¦†ç›–å¹¶ç¦ç”¨æ¢å¤
        # è®¾ç½®ä¿å­˜ç›®å½•å¹¶é€’å¢è·¯å¾„
        opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))

    # DDPæ¨¡å¼ï¼ˆåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼‰
    device = select_device(opt.device, batch_size=opt.batch_size)  # é€‰æ‹©è®¡ç®—è®¾å¤‡
    if LOCAL_RANK != -1:  # å¦‚æœå¯ç”¨DDP
        assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'  # æ£€æŸ¥CUDAè®¾å¤‡æ•°é‡æ˜¯å¦è¶³å¤Ÿ
        assert opt.batch_size % WORLD_SIZE == 0, '--batch-size must be multiple of CUDA device count'  # ç¡®ä¿batch sizeæ˜¯è®¾å¤‡æ•°çš„å€æ•°
        assert not opt.image_weights, '--image-weights argument is not compatible with DDP training'  # ç¡®ä¿æœªå¯ç”¨å›¾åƒæƒé‡
        assert not opt.evolve, '--evolve argument is not compatible with DDP training'  # ç¡®ä¿æœªå¯ç”¨æ¼”åŒ–æ¨¡å¼
        torch.cuda.set_device(LOCAL_RANK)  # è®¾ç½®CUDAè®¾å¤‡
        device = torch.device('cuda', LOCAL_RANK)  # æŒ‡å®šè®¾å¤‡
        dist.init_process_group(backend="nccl" if dist.is_nccl_available() else "gloo")  # åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼Œé€‰æ‹©ncclæˆ–glooä½œä¸ºé€šä¿¡åç«¯

    # è®­ç»ƒæ¨¡å‹
    if not opt.evolve:  # å¦‚æœä¸æ˜¯æ¼”åŒ–æ¨¡å¼ï¼Œè¿›è¡Œè®­ç»ƒ
        train(opt.hyp, opt, device, callbacks)  # è°ƒç”¨trainå‡½æ•°è¿›è¡Œè®­ç»ƒ
        if WORLD_SIZE > 1 and RANK == 0:  # åœ¨å¤šGPUæ¨¡å¼ä¸‹ï¼Œé”€æ¯è¿›ç¨‹ç»„
            LOGGER.info('Destroying process group... ')
            dist.destroy_process_group()  # é”€æ¯DDPè¿›ç¨‹ç»„

    # è¿›è¡Œè¶…å‚æ•°æ¼”åŒ–ï¼ˆå¯é€‰ï¼‰
    else:
        # è¶…å‚æ•°æ¼”åŒ–å…ƒæ•°æ®ï¼ˆå˜å¼‚è§„æ¨¡ 0-1ï¼Œæœ€å°å€¼ï¼Œä¸Šé™ï¼‰
        meta = {
            'lr0': (1, 1e-5, 1e-1),  # åˆå§‹å­¦ä¹ ç‡ (SGD=1E-2, Adam=1E-3)
            'lrf': (1, 0.01, 1.0),  # æœ€ç»ˆ OneCycleLR å­¦ä¹ ç‡ (lr0 * lrf)
            'momentum': (0.3, 0.6, 0.98),  # SGD åŠ¨é‡/Adam beta1
            'weight_decay': (1, 0.0, 0.001),  # ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡
            'warmup_epochs': (1, 0.0, 5.0),  # é¢„çƒ­è½®æ•°ï¼ˆå…è®¸ä½¿ç”¨å°æ•°ï¼‰
            'warmup_momentum': (1, 0.0, 0.95),  # é¢„çƒ­åˆå§‹åŠ¨é‡
            'warmup_bias_lr': (1, 0.0, 0.2),  # é¢„çƒ­åˆå§‹åç½®å­¦ä¹ ç‡
            'box': (1, 0.02, 0.2),  # è¾¹æ¡†æŸå¤±å¢ç›Š
            'cls': (1, 0.2, 4.0),  # åˆ†ç±»æŸå¤±å¢ç›Š
            'cls_pw': (1, 0.5, 2.0),  # åˆ†ç±» BCELoss æ­£æƒé‡
            'obj': (1, 0.2, 4.0),  # ç›®æ ‡æŸå¤±å¢ç›Šï¼ˆä¸åƒç´ æˆæ¯”ä¾‹ï¼‰
            'obj_pw': (1, 0.5, 2.0),  # ç›®æ ‡ BCELoss æ­£æƒé‡
            'iou_t': (0, 0.1, 0.7),  # IoU è®­ç»ƒé˜ˆå€¼
            'anchor_t': (1, 2.0, 8.0),  # é”šç‚¹å€æ•°é˜ˆå€¼
            'anchors': (2, 2.0, 10.0),  # æ¯ä¸ªè¾“å‡ºç½‘æ ¼çš„é”šç‚¹æ•°é‡ï¼ˆ0 ä¸ºå¿½ç•¥ï¼‰
            'fl_gamma': (0, 0.0, 2.0),  # èšç„¦æŸå¤±ä¼½é©¬ï¼ˆefficientDet é»˜è®¤ä¼½é©¬=1.5ï¼‰
            'hsv_h': (1, 0.0, 0.1),  # å›¾åƒ HSV-è‰²ç›¸å¢å¼ºï¼ˆæ¯”ä¾‹ï¼‰
            'hsv_s': (1, 0.0, 0.9),  # å›¾åƒ HSV-é¥±å’Œåº¦å¢å¼ºï¼ˆæ¯”ä¾‹ï¼‰
            'hsv_v': (1, 0.0, 0.9),  # å›¾åƒ HSV-äº®åº¦å¢å¼ºï¼ˆæ¯”ä¾‹ï¼‰
            'degrees': (1, 0.0, 45.0),  # å›¾åƒæ—‹è½¬ (+/- åº¦)
            'translate': (1, 0.0, 0.9),  # å›¾åƒå¹³ç§» (+/- æ¯”ä¾‹)
            'scale': (1, 0.0, 0.9),  # å›¾åƒç¼©æ”¾ (+/- å¢ç›Š)
            'shear': (1, 0.0, 10.0),  # å›¾åƒå‰ªåˆ‡ (+/- åº¦)
            'perspective': (0, 0.0, 0.001),  # å›¾åƒé€è§† (+/- æ¯”ä¾‹)ï¼ŒèŒƒå›´ 0-0.001
            'flipud': (1, 0.0, 1.0),  # å›¾åƒä¸Šä¸‹ç¿»è½¬ï¼ˆæ¦‚ç‡ï¼‰
            'fliplr': (0, 0.0, 1.0),  # å›¾åƒå·¦å³ç¿»è½¬ï¼ˆæ¦‚ç‡ï¼‰
            'mosaic': (1, 0.0, 1.0),  # å›¾åƒæ··åˆï¼ˆæ¦‚ç‡ï¼‰
            'mixup': (1, 0.0, 1.0),  # å›¾åƒæ··åˆï¼ˆæ¦‚ç‡ï¼‰
            'copy_paste': (1, 0.0, 1.0)  # æ®µè½å¤åˆ¶ç²˜è´´ï¼ˆæ¦‚ç‡ï¼‰
        }

        # æ‰“å¼€è¶…å‚æ•°æ–‡ä»¶å¹¶åŠ è½½è¶…å‚æ•°å­—å…¸
        with open(opt.hyp, errors='ignore') as f:
            hyp = yaml.safe_load(f)  # ä½¿ç”¨ YAML åŠ è½½è¶…å‚æ•°å­—å…¸
            if 'anchors' not in hyp:  # å¦‚æœè¶…å‚æ•°ä¸­æ²¡æœ‰ anchorsï¼ˆå¯èƒ½è¢«æ³¨é‡Šæ‰ï¼‰
                hyp['anchors'] = 3  # è®¾ç½®é»˜è®¤çš„ anchors æ•°é‡

        # è®¾ç½®é€‰é¡¹ï¼ŒæŒ‡ç¤ºåªè¿›è¡ŒéªŒè¯å’Œä¿å­˜æœ€ç»ˆçš„è®­ç»ƒç»“æœ
        opt.noval, opt.nosave, save_dir = True, True, Path(opt.save_dir)  # åªéªŒè¯å’Œä¿å­˜æœ€ç»ˆè½®æ¬¡çš„æ¨¡å‹

        # å®šä¹‰æ¼”åŒ–æ–‡ä»¶è·¯å¾„
        evolve_yaml, evolve_csv = save_dir / 'hyp_evolve.yaml', save_dir / 'evolve.csv'
        if opt.bucket:  # å¦‚æœæŒ‡å®šäº†äº‘å­˜å‚¨æ¡¶
            # ä¸‹è½½å·²æœ‰çš„ evolve.csv æ–‡ä»¶
            os.system(f'gsutil cp gs://{opt.bucket}/evolve.csv {save_dir}')  # ä¸‹è½½ evolve.csvï¼ˆå¦‚æœå­˜åœ¨ï¼‰

        # è¿›è¡ŒæŒ‡å®šè½®æ•°çš„è¶…å‚æ•°æ¼”åŒ–
        for _ in range(opt.evolve):  # è¿­ä»£æ¼”åŒ–çš„ä»£æ•°
            if evolve_csv.exists():  # å¦‚æœ evolve.csv å­˜åœ¨ï¼Œé€‰æ‹©æœ€ä½³è¶…å‚æ•°å¹¶è¿›è¡Œå˜å¼‚
                # é€‰æ‹©çˆ¶ä»£
                parent = 'single'  # çˆ¶ä»£é€‰æ‹©æ–¹æ³•ï¼š'single' æˆ– 'weighted'
                x = np.loadtxt(evolve_csv, ndmin=2, delimiter=',', skiprows=1)  # åŠ è½½æ¼”åŒ–ç»“æœ
                n = min(5, len(x))  # è€ƒè™‘çš„ä¸Šä¸€ä¸ªç»“æœçš„æ•°é‡
                x = x[np.argsort(-fitness(x))][:n]  # æŒ‰é€‚åº”åº¦æ’åºï¼Œé€‰æ‹©å‰ n ä¸ªå˜å¼‚
                w = fitness(x) - fitness(x).min() + 1E-6  # è®¡ç®—æƒé‡ï¼ˆç¡®ä¿å’Œå¤§äº0ï¼‰

                # æ ¹æ®é€‰æ‹©æ–¹æ³•é€‰å–çˆ¶ä»£
                if parent == 'single' or len(x) == 1:
                    # x = x[random.randint(0, n - 1)]  # éšæœºé€‰æ‹©
                    x = x[random.choices(range(n), weights=w)[0]]  # åŸºäºæƒé‡é€‰æ‹©
                elif parent == 'weighted':
                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # åŠ æƒç»„åˆ

                # è¿›è¡Œå˜å¼‚
                mp, s = 0.8, 0.2  # å˜å¼‚æ¦‚ç‡ï¼Œæ ‡å‡†å·®
                npr = np.random
                npr.seed(int(time.time()))  # è®¾ç½®éšæœºç§å­
                g = np.array([meta[k][0] for k in hyp.keys()])  # è·å–å¢ç›Šï¼ŒèŒƒå›´ 0-1
                ng = len(meta)  # å…ƒæ•°æ®ä¸­çš„è¶…å‚æ•°æ•°é‡
                v = np.ones(ng)  # åˆå§‹åŒ–å˜å¼‚é‡

                # ç¡®ä¿å˜å¼‚å‘ç”Ÿï¼Œé¿å…é‡å¤
                while all(v == 1):  # åœ¨æ²¡æœ‰å˜åŒ–æ—¶ç»§ç»­å˜å¼‚
                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)

                # åº”ç”¨å˜å¼‚
                for i, k in enumerate(hyp.keys()):  # éå†è¶…å‚æ•°
                    hyp[k] = float(x[i + 7] * v[i])  # å˜å¼‚è¶…å‚æ•°

            # é™åˆ¶è¶…å‚æ•°åœ¨é¢„è®¾èŒƒå›´å†…
            for k, v in meta.items():
                hyp[k] = max(hyp[k], v[1])  # é™åˆ¶ä¸‹é™
                hyp[k] = min(hyp[k], v[2])  # é™åˆ¶ä¸Šé™
                hyp[k] = round(hyp[k], 5)  # ä¿ç•™äº”ä½æœ‰æ•ˆæ•°å­—

            # è®­ç»ƒå˜å¼‚åçš„æ¨¡å‹
            results = train(hyp.copy(), opt, device, callbacks)

            # å†™å…¥å˜å¼‚ç»“æœ
            print_mutation(results, hyp.copy(), save_dir, opt.bucket)

        # ç»˜åˆ¶ç»“æœå›¾è¡¨
        plot_evolve(evolve_csv)
        print(f'Hyperparameter evolution finished\n'
              f"Results saved to {colorstr('bold', save_dir)}\n"
              f'Use best hyperparameters example: $ python train.py --hyp {evolve_yaml}')

def run(**kwargs):
    # ç”¨æ³•ç¤ºä¾‹: import train; train.run(data='coco128.yaml', imgsz=320, weights='yolov5m.pt')
    opt = parse_opt(True)  # è§£æå‘½ä»¤è¡Œå‚æ•°å¹¶è¿”å›é€‰é¡¹å¯¹è±¡
    for k, v in kwargs.items():  # éå†å…³é”®å­—å‚æ•°
        setattr(opt, k, v)  # å°†æ¯ä¸ªå‚æ•°è®¾ç½®åˆ°é€‰é¡¹å¯¹è±¡ä¸­
    main(opt)  # è°ƒç”¨ä¸»å‡½æ•°ï¼Œä¼ å…¥é€‰é¡¹å¯¹è±¡

if __name__ == "__main__":
    opt = parse_opt()
    main(opt)
