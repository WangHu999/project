# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Auto-anchor utils
"""

import random

import numpy as np
import torch
import yaml
from tqdm import tqdm

from utils.general import colorstr


def check_anchor_order(m):
    # æ£€æŸ¥ YOLOv5 Detect() æ¨¡å— m ä¸­çš„é”šæ¡†é¡ºåºæ˜¯å¦ä¸æ­¥å¹…é¡ºåºä¸€è‡´ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œä¿®æ­£
    a = m.anchors.prod(-1).view(-1)  # è®¡ç®—é”šæ¡†çš„é¢ç§¯
    da = a[-1] - a[0]  # è®¡ç®—é”šæ¡†é¢ç§¯çš„å·®å€¼
    ds = m.stride[-1] - m.stride[0]  # è®¡ç®—æ­¥å¹…çš„å·®å€¼
    if da.sign() != ds.sign():  # å¦‚æœé¢ç§¯å’Œæ­¥å¹…çš„é¡ºåºä¸ä¸€è‡´
        print('Reversing anchor order')  # æ‰“å°æç¤ºä¿¡æ¯
        m.anchors[:] = m.anchors.flip(0)  # åè½¬é”šæ¡†é¡ºåº


def check_anchors(dataset, model, thr=4.0, imgsz=640):
    # æ£€æŸ¥é”šç‚¹æ˜¯å¦é€‚åˆæ•°æ®ï¼Œå¦‚æœ‰å¿…è¦åˆ™é‡æ–°è®¡ç®—
    prefix = colorstr('autoanchor: ')
    print(f'\n{prefix}Analyzing anchors... ', end='')

    # è·å–æ¨¡å‹çš„æ£€æµ‹å±‚
    m = model.module.model[-1] if hasattr(model, 'module') else model.model[-1]  # Detect()

    # è®¡ç®—å›¾åƒå°ºå¯¸çš„æ¯”ä¾‹
    shapes = imgsz * dataset.shapes / dataset.shapes.max(1, keepdims=True)

    # éšæœºç¼©æ”¾å› å­
    scale = np.random.uniform(0.9, 1.1, size=(shapes.shape[0], 1))  # augment scale

    # è®¡ç®—å®½é«˜
    wh = torch.tensor(np.concatenate([l[:, 3:5] * s for s, l in zip(shapes * scale, dataset.labels)])).float()  # wh

    def metric(k):  # è®¡ç®—æŒ‡æ ‡
        r = wh[:, None] / k[None]  # è®¡ç®—æ¯”ä¾‹
        x = torch.min(r, 1. / r).min(2)[0]  # æ¯”ä¾‹æŒ‡æ ‡
        best = x.max(1)[0]  # æœ€ä½³æ¯”ä¾‹
        aat = (x > 1. / thr).float().sum(1).mean()  # è¶…è¿‡é˜ˆå€¼çš„é”šç‚¹æ•°é‡
        bpr = (best > 1. / thr).float().mean()  # æœ€ä½³å¯èƒ½å¬å›ç‡
        return bpr, aat

    # è·å–å½“å‰é”šç‚¹å¹¶è€ƒè™‘æ¨¡å‹çš„æ­¥å¹…
    anchors = m.anchors.clone() * m.stride.to(m.anchors.device).view(-1, 1, 1)  # å½“å‰é”šç‚¹
    bpr, aat = metric(anchors.cpu().view(-1, 2))  # è®¡ç®—æŒ‡æ ‡
    print(f'anchors/target = {aat:.2f}, Best Possible Recall (BPR) = {bpr:.4f}', end='')

    # å¦‚æœæœ€ä½³å¯èƒ½å¬å›ç‡ä½äºé˜ˆå€¼ï¼Œåˆ™å°è¯•æ”¹å–„é”šç‚¹
    if bpr < 0.98:  # threshold to recompute
        print('. Attempting to improve anchors, please wait...')
        na = m.anchors.numel() // 2  # é”šç‚¹æ•°é‡
        try:
            anchors = kmean_anchors(dataset, n=na, img_size=imgsz, thr=thr, gen=1000, verbose=False)
        except Exception as e:
            print(f'{prefix}ERROR: {e}')
        new_bpr = metric(anchors)[0]
        if new_bpr > bpr:  # å¦‚æœæ–°é”šç‚¹æ›´å¥½ï¼Œåˆ™æ›¿æ¢
            anchors = torch.tensor(anchors, device=m.anchors.device).type_as(m.anchors)
            m.anchors[:] = anchors.clone().view_as(m.anchors) / m.stride.to(m.anchors.device).view(-1, 1, 1)  # loss
            check_anchor_order(m)  # æ£€æŸ¥é”šç‚¹é¡ºåº
            print(f'{prefix}New anchors saved to model. Update model *.yaml to use these anchors in the future.')
        else:
            print(f'{prefix}Original anchors better than new anchors. Proceeding with original anchors.')

    print('')  # æ¢è¡Œ


def kmean_anchors(dataset='./data/coco128.yaml', n=9, img_size=640, thr=4.0, gen=1000, verbose=True):
    """ åˆ›å»ºç»è¿‡kmeansè¿›åŒ–çš„é”šç‚¹

        å‚æ•°:
            dataset: æ•°æ®é›†çš„è·¯å¾„æˆ–å·²åŠ è½½çš„æ•°æ®é›†
            n: é”šç‚¹çš„æ•°é‡
            img_size: ç”¨äºè®­ç»ƒçš„å›¾åƒå°ºå¯¸
            thr: ç”¨äºè®­ç»ƒçš„é”šç‚¹-æ ‡ç­¾å®½é«˜æ¯”é˜ˆå€¼ï¼Œé»˜è®¤ä¸º4.0
            gen: ä½¿ç”¨é—ä¼ ç®—æ³•è¿›åŒ–é”šç‚¹çš„ä»£æ•°
            verbose: æ˜¯å¦æ‰“å°æ‰€æœ‰ç»“æœ

        è¿”å›:
            k: kmeansè¿›åŒ–åçš„é”šç‚¹

        ç”¨æ³•:
            from utils.autoanchor import *; _ = kmean_anchors()
    """
    from scipy.cluster.vq import kmeans  # å¯¼å…¥kmeanså‡½æ•°

    thr = 1. / thr  # å°†é˜ˆå€¼åè½¬ï¼Œä»¥ä¾¿äºåç»­æ¯”è¾ƒ
    prefix = colorstr('autoanchor: ')  # è®¾ç½®æ‰“å°å‰ç¼€

    def metric(k, wh):  # è®¡ç®—æŒ‡æ ‡
        r = wh[:, None] / k[None]  # è®¡ç®—å®½é«˜æ¯”
        x = torch.min(r, 1. / r).min(2)[0]  # è·å–æ¯”ä¾‹æŒ‡æ ‡
        return x, x.max(1)[0]  # è¿”å›æ¯”ä¾‹å’Œæœ€ä½³æ¯”ä¾‹

    def anchor_fitness(k):  # è®¡ç®—é”šç‚¹çš„é€‚åº”åº¦
        _, best = metric(torch.tensor(k, dtype=torch.float32), wh)  # è®¡ç®—å½“å‰é”šç‚¹çš„æŒ‡æ ‡
        return (best * (best > thr).float()).mean()  # è®¡ç®—é€‚åº”åº¦ï¼Œåªæœ‰æ»¡è¶³é˜ˆå€¼çš„æ‰è®¡å…¥

    def print_results(k):  # æ‰“å°ç»“æœ
        k = k[np.argsort(k.prod(1))]  # æŒ‰é¢ç§¯ä»å°åˆ°å¤§æ’åºé”šç‚¹
        x, best = metric(k, wh0)  # è®¡ç®—å½“å‰é”šç‚¹çš„æŒ‡æ ‡
        bpr, aat = (best > thr).float().mean(), (x > thr).float().mean() * n  # è®¡ç®—æœ€ä½³å¯èƒ½å¬å›ç‡
        print(f'{prefix}thr={thr:.2f}: {bpr:.4f} best possible recall, {aat:.2f} anchors past thr')  # æ‰“å°å¬å›ç‡
        print(f'{prefix}n={n}, img_size={img_size}, metric_all={x.mean():.3f}/{best.mean():.3f}-mean/best, '
              f'past_thr={x[x > thr].mean():.3f}-mean: ', end='')  # æ‰“å°å„ç±»æŒ‡æ ‡
        for i, x in enumerate(k):
            print('%i,%i' % (round(x[0]), round(x[1])), end=',  ' if i < len(k) - 1 else '\n')  # è¾“å‡ºé”šç‚¹çš„å°ºå¯¸
        return k  # è¿”å›é”šç‚¹

    if isinstance(dataset, str):  # å¦‚æœè¾“å…¥çš„æ˜¯æ–‡ä»¶è·¯å¾„
        with open(dataset, errors='ignore') as f:  # ä»¥å¿½ç•¥é”™è¯¯çš„æ–¹å¼æ‰“å¼€æ–‡ä»¶
            data_dict = yaml.safe_load(f)  # è¯»å–æ•°æ®å­—å…¸
        from utils.datasets import LoadImagesAndLabels  # å¯¼å…¥æ•°æ®åŠ è½½å·¥å…·
        dataset = LoadImagesAndLabels(data_dict['train'], augment=True, rect=True)  # åŠ è½½è®­ç»ƒæ•°æ®é›†

    # è·å–æ ‡ç­¾çš„å®½é«˜
    shapes = img_size * dataset.shapes / dataset.shapes.max(1, keepdims=True)  # è®¡ç®—æ¯ä¸ªå›¾åƒçš„å½¢çŠ¶æ¯”ä¾‹
    wh0 = np.concatenate([l[:, 3:5] * s for s, l in zip(shapes, dataset.labels)])  # åˆå¹¶æ‰€æœ‰æ ‡ç­¾çš„å®½é«˜

    # è¿‡æ»¤æå°ç‰©ä½“
    i = (wh0 < 3.0).any(1).sum()  # ç»Ÿè®¡å°äº3åƒç´ çš„æ ‡ç­¾æ•°é‡
    if i:
        print(f'{prefix}WARNING: Extremely small objects found. {i} of {len(wh0)} labels are < 3 pixels in size.')  # è­¦å‘Šä¿¡æ¯
    wh = wh0[(wh0 >= 2.0).any(1)]  # è¿‡æ»¤æ‰å°äº2åƒç´ çš„æ ‡ç­¾

    # Kmeansè®¡ç®—
    print(f'{prefix}Running kmeans for {n} anchors on {len(wh)} points...')  # å¼€å§‹Kmeansè®¡ç®—
    s = wh.std(0)  # è®¡ç®—å®½é«˜çš„æ ‡å‡†å·®
    k, dist = kmeans(wh / s, n, iter=30)  # æ‰§è¡ŒKmeansèšç±»
    assert len(k) == n, f'{prefix}ERROR: scipy.cluster.vq.kmeans requested {n} points but returned only {len(k)}'  # ç¡®ä¿è¿”å›çš„é”šç‚¹æ•°é‡æ­£ç¡®
    k *= s  # è¿˜åŸé”šç‚¹
    wh = torch.tensor(wh, dtype=torch.float32)  # è½¬æ¢ä¸ºå¼ é‡
    wh0 = torch.tensor(wh0, dtype=torch.float32)  # åŸå§‹æ•°æ®è½¬å¼ é‡
    k = print_results(k)  # æ‰“å°é”šç‚¹ç»“æœ

    # è¿›åŒ–é”šç‚¹
    npr = np.random  # å¼•ç”¨éšæœºæ•°ç”Ÿæˆå™¨
    f, sh, mp, s = anchor_fitness(k), k.shape, 0.9, 0.1  # åˆå§‹åŒ–é€‚åº”åº¦å’Œå‚æ•°
    pbar = tqdm(range(gen), desc=f'{prefix}Evolving anchors with Genetic Algorithm:')  # è¿›åº¦æ¡
    for _ in pbar:
        v = np.ones(sh)  # åˆå§‹åŒ–å˜å¼‚å‘é‡
        while (v == 1).all():  # å˜å¼‚ç›´åˆ°æœ‰å˜åŒ–
            v = ((npr.random(sh) < mp) * random.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0)  # ç”Ÿæˆå˜å¼‚
        kg = (k.copy() * v).clip(min=2.0)  # ç”Ÿæˆæ–°é”šç‚¹
        fg = anchor_fitness(kg)  # è®¡ç®—æ–°é”šç‚¹çš„é€‚åº”åº¦
        if fg > f:  # å¦‚æœæ–°é€‚åº”åº¦æ›´é«˜
            f, k = fg, kg.copy()  # æ›´æ–°é€‚åº”åº¦å’Œé”šç‚¹
            pbar.desc = f'{prefix}Evolving anchors with Genetic Algorithm: fitness = {f:.4f}'  # æ›´æ–°è¿›åº¦æ¡æè¿°
            if verbose:
                print_results(k)  # æ‰“å°æ–°é”šç‚¹ç»“æœ

    return print_results(k)  # è¿”å›æœ€ç»ˆçš„é”šç‚¹ç»“æœ
