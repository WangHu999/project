# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Dataloaders and dataset utils
"""

import glob
import hashlib
import json
import logging
import os
import random
import shutil
import time
from itertools import repeat
from multiprocessing.pool import ThreadPool, Pool
from pathlib import Path
from threading import Thread
from zipfile import ZipFile

import cv2
import numpy as np
import torch
import torch.nn.functional as F
import yaml
from PIL import Image, ExifTags
from torch.utils.data import Dataset
from tqdm import tqdm

from utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective
from utils.general import check_dataset, check_requirements, check_yaml, clean_str, segments2boxes, \
    xywh2xyxy, xywhn2xyxy, xyxy2xywhn, xyn2xy
from utils.torch_utils import torch_distributed_zero_first

# Parameters
HELP_URL = 'https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data'  # æä¾›çš„å¸®åŠ©æ–‡æ¡£é“¾æ¥
IMG_FORMATS = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']  # å¯æ¥å—çš„å›¾åƒæ–‡ä»¶åç¼€
VID_FORMATS = ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']  # å¯æ¥å—çš„è§†é¢‘æ–‡ä»¶åç¼€
NUM_THREADS = min(8, os.cpu_count())  # ä½¿ç”¨çš„å¤šçº¿ç¨‹æ•°é‡ï¼Œæœ€å¤šä¸º8ä¸ªæˆ–CPUæ ¸å¿ƒæ•°é‡çš„æœ€å°å€¼

# è·å–Exifä¸­çš„æ–¹å‘æ ‡ç­¾
for orientation in ExifTags.TAGS.keys():
    if ExifTags.TAGS[orientation] == 'Orientation':  # æŸ¥æ‰¾â€œOrientationâ€æ ‡ç­¾
        break  # æ‰¾åˆ°åé€€å‡ºå¾ªç¯


def get_hash(paths):
    """
    è¿”å›æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„åˆ—è¡¨çš„å•ä¸ªå“ˆå¸Œå€¼

    å‚æ•°:
        paths: æ–‡ä»¶æˆ–ç›®å½•çš„è·¯å¾„åˆ—è¡¨

    è¿”å›:
        h: è®¡ç®—å‡ºçš„å“ˆå¸Œå€¼
    """
    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # è®¡ç®—æ‰€æœ‰è·¯å¾„çš„å¤§å°æ€»å’Œ
    h = hashlib.md5(str(size).encode())  # å¯¹å¤§å°è¿›è¡ŒMD5å“ˆå¸Œ
    h.update(''.join(paths).encode())  # å¯¹è·¯å¾„è¿›è¡ŒMD5å“ˆå¸Œ
    return h.hexdigest()  # è¿”å›æœ€ç»ˆçš„å“ˆå¸Œå€¼


def exif_size(img):
    """
    è¿”å›ç»è¿‡Exifæ ¡æ­£çš„PILå›¾åƒå¤§å°

    å‚æ•°:
        img: PILå›¾åƒå¯¹è±¡

    è¿”å›:
        s: æ ¡æ­£åçš„å›¾åƒå¤§å°å…ƒç»„ (å®½åº¦, é«˜åº¦)
    """
    s = img.size  # è·å–å›¾åƒçš„åŸå§‹å¤§å° (å®½åº¦, é«˜åº¦)
    try:
        # ä»å›¾åƒçš„Exifä¿¡æ¯ä¸­è·å–æ–¹å‘å€¼
        rotation = dict(img._getexif().items())[orientation]
        if rotation == 6:  # å¦‚æœæ—‹è½¬å€¼ä¸º6ï¼ˆé¡ºæ—¶é’ˆ90åº¦ï¼‰
            s = (s[1], s[0])  # äº¤æ¢å®½é«˜
        elif rotation == 8:  # å¦‚æœæ—‹è½¬å€¼ä¸º8ï¼ˆé€†æ—¶é’ˆ90åº¦ï¼‰
            s = (s[1], s[0])  # äº¤æ¢å®½é«˜
    except:
        pass  # å¦‚æœæ²¡æœ‰Exifä¿¡æ¯æˆ–å‘ç”Ÿé”™è¯¯ï¼Œä¿æŒåŸå§‹å¤§å°

    return s  # è¿”å›æ ¡æ­£åçš„å¤§å°



def exif_transpose(image):
    """
    æ ¹æ®å›¾åƒçš„EXIFæ–¹å‘æ ‡ç­¾å¯¹PILå›¾åƒè¿›è¡Œè½¬ç½®ã€‚
    æ¥æºï¼šhttps://github.com/python-pillow/Pillow/blob/master/src/PIL/ImageOps.py

    :param image: éœ€è¦è½¬ç½®çš„å›¾åƒã€‚
    :return: å¤„ç†åçš„å›¾åƒã€‚
    """
    exif = image.getexif()  # è·å–å›¾åƒçš„EXIFä¿¡æ¯
    orientation = exif.get(0x0112, 1)  # è·å–æ–¹å‘æ ‡ç­¾ï¼ˆé»˜è®¤ä¸º1ï¼Œå³æ— æ—‹è½¬ï¼‰

    if orientation > 1:  # å¦‚æœæ–¹å‘æ ‡ç­¾å¤§äº1ï¼Œè¯´æ˜å›¾åƒéœ€è¦æ—‹è½¬æˆ–ç¿»è½¬
        # æ ¹æ®æ–¹å‘æ ‡ç­¾é€‰æ‹©ç›¸åº”çš„è½¬ç½®æ–¹æ³•
        method = {2: Image.FLIP_LEFT_RIGHT,    # æ°´å¹³ç¿»è½¬
                  3: Image.ROTATE_180,       # 180åº¦æ—‹è½¬
                  4: Image.FLIP_TOP_BOTTOM,   # å‚ç›´ç¿»è½¬
                  5: Image.TRANSPOSE,         # å·¦ä¸Šåˆ°å³ä¸‹å¯¹è§’çº¿ç¿»è½¬
                  6: Image.ROTATE_270,       # é¡ºæ—¶é’ˆæ—‹è½¬270åº¦
                  7: Image.TRANSVERSE,       # å³ä¸Šåˆ°å·¦ä¸‹å¯¹è§’çº¿ç¿»è½¬
                  8: Image.ROTATE_90,        # é¡ºæ—¶é’ˆæ—‹è½¬90åº¦
                  }.get(orientation)  # æ ¹æ®æ–¹å‘æ ‡ç­¾è·å–è½¬ç½®æ–¹æ³•

        if method is not None:  # å¦‚æœæ‰¾åˆ°äº†å¯¹åº”çš„è½¬ç½®æ–¹æ³•
            image = image.transpose(method)  # å¯¹å›¾åƒè¿›è¡Œè½¬ç½®
            del exif[0x0112]  # åˆ é™¤æ–¹å‘æ ‡ç­¾ï¼Œå› ä¸ºå·²å¤„ç†
            image.info["exif"] = exif.tobytes()  # æ›´æ–°å›¾åƒçš„EXIFä¿¡æ¯

    return image  # è¿”å›å¤„ç†åçš„å›¾åƒ



def create_dataloader(
    path, imgsz, batch_size, stride, single_cls=False, hyp=None,
    augment=False, cache=False, pad=0.0, rect=False, rank=-1,
    workers=8, image_weights=False, quad=False, prefix=''
):
    # ç¡®ä¿åªæœ‰ç¬¬ä¸€ä¸ªè¿›ç¨‹åœ¨ DDP ä¸­é¦–å…ˆå¤„ç†æ•°æ®é›†ï¼Œå…¶ä»–è¿›ç¨‹å¯ä»¥ä½¿ç”¨ç¼“å­˜
    with torch_distributed_zero_first(rank):
        dataset = LoadImagesAndLabels(
            path, imgsz, batch_size,
            augment=augment,  # æ˜¯å¦è¿›è¡Œå›¾åƒå¢å¼º
            hyp=hyp,  # å¢å¼ºçš„è¶…å‚æ•°
            rect=rect,  # æ˜¯å¦è¿›è¡ŒçŸ©å½¢è®­ç»ƒ
            cache_images=cache,  # æ˜¯å¦ç¼“å­˜å›¾åƒ
            single_cls=single_cls,  # æ˜¯å¦ä¸ºå•ç±»åˆ«æ£€æµ‹
            stride=int(stride),  # æ­¥å¹…
            pad=pad,  # å¡«å……
            image_weights=image_weights,  # æ˜¯å¦ä½¿ç”¨å›¾åƒåŠ æƒ
            prefix=prefix  # æ—¥å¿—å‰ç¼€
        )

    batch_size = min(batch_size, len(dataset))  # ç¡®ä¿æ‰¹å¤§å°ä¸è¶…è¿‡æ•°æ®é›†å¤§å°
    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, workers])  # è®¡ç®—å·¥ä½œçº¿ç¨‹æ•°
    sampler = torch.utils.data.distributed.DistributedSampler(dataset) if rank != -1 else None  # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    loader = torch.utils.data.DataLoader if image_weights else InfiniteDataLoader  # é€‰æ‹©åŠ è½½å™¨

    # ä½¿ç”¨ torch.utils.data.DataLoader() å¦‚æœæ•°æ®é›†å±æ€§åœ¨è®­ç»ƒæœŸé—´ä¼šæ›´æ–°ï¼Œå¦åˆ™ä½¿ç”¨ InfiniteDataLoader()
    dataloader = loader(
        dataset,
        batch_size=batch_size,
        num_workers=nw,
        sampler=sampler,
        pin_memory=True,
        collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn
    )
    return dataloader, dataset  # è¿”å›æ•°æ®åŠ è½½å™¨å’Œæ•°æ®é›†



class InfiniteDataLoader(torch.utils.data.dataloader.DataLoader):
    """
    ä¸€ä¸ªå¯é‡å¤ä½¿ç”¨å·¥ä½œçº¿ç¨‹çš„Dataloader

    é‡‡ç”¨ä¸æ™®é€šDataLoaderç›¸åŒçš„è¯­æ³•ã€‚
    """

    def __init__(self, *args, **kwargs):
        """
        åˆå§‹åŒ–InfiniteDataLoaderå®ä¾‹ã€‚

        Args:
            *args: ä¼ é€’ç»™çˆ¶ç±»DataLoaderçš„å‚æ•°ã€‚
            **kwargs: ä¼ é€’ç»™çˆ¶ç±»DataLoaderçš„å…³é”®å­—å‚æ•°ã€‚
        """
        super().__init__(*args, **kwargs)  # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–æ–¹æ³•
        object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))  # å°†batch_sampleræ›¿æ¢ä¸º_repeat_sampler
        self.iterator = super().__iter__()  # è·å–çˆ¶ç±»çš„è¿­ä»£å™¨

    def __len__(self):
        """
        è¿”å›æ ·æœ¬çš„æ•°é‡ã€‚

        Returns:
            int: batch_samplerä¸­çš„æ ·æœ¬æ•°é‡ã€‚
        """
        return len(self.batch_sampler.sampler)

    def __iter__(self):
        """
        è¿­ä»£å™¨æ–¹æ³•ï¼Œæ”¯æŒæ— é™è¿­ä»£ã€‚

        Yields:
            è¿”å›æ¯æ¬¡è¿­ä»£çš„ä¸‹ä¸€ä¸ªæ•°æ®æ‰¹æ¬¡ã€‚
        """
        for i in range(len(self)):
            yield next(self.iterator)  # ä»çˆ¶ç±»è¿­ä»£å™¨ä¸­è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡


class _RepeatSampler(object):
    """
    ä¸€ä¸ªæ— é™é‡å¤çš„Sampler

    Args:
        sampler (Sampler): ç”¨äºç”Ÿæˆæ ·æœ¬çš„åŸå§‹é‡‡æ ·å™¨ã€‚
    """

    def __init__(self, sampler):
        """
        åˆå§‹åŒ–_repeat_samplerå®ä¾‹ã€‚

        Args:
            sampler (Sampler): ä¼ å…¥çš„æ ·æœ¬é‡‡æ ·å™¨ã€‚
        """
        self.sampler = sampler  # ä¿å­˜ä¼ å…¥çš„é‡‡æ ·å™¨

    def __iter__(self):
        """
        æ— é™è¿­ä»£å™¨æ–¹æ³•ï¼Œæ”¯æŒå¯¹æ ·æœ¬çš„æ— é™é‡å¤é‡‡æ ·ã€‚

        Yields:
            ä»samplerä¸­ç”Ÿæˆçš„æ ·æœ¬ã€‚
        """
        while True:
            yield from iter(self.sampler)  # æ— é™è¿”å›é‡‡æ ·å™¨ä¸­çš„æ ·æœ¬



class LoadImages:
    # YOLOv5 å›¾åƒ/è§†é¢‘æ•°æ®åŠ è½½å™¨ï¼Œç¤ºä¾‹ç”¨æ³•ï¼š`python detect.py --source image.jpg/vid.mp4`

    def __init__(self, path, img_size=640, stride=32, auto=True):
        # å°†è·¯å¾„è½¬æ¢ä¸ºæ“ä½œç³»ç»Ÿæ— å…³çš„ç»å¯¹è·¯å¾„
        p = str(Path(path).resolve())

        # æ£€æŸ¥è·¯å¾„æ˜¯å¦åŒ…å«é€šé…ç¬¦ï¼Œå¹¶ä½¿ç”¨ glob æ¨¡å—æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
        if '*' in p:
            files = sorted(glob.glob(p, recursive=True))  # ä½¿ç”¨é€šé…ç¬¦æŸ¥æ‰¾æ–‡ä»¶
        # å¦‚æœè·¯å¾„æ˜¯ç›®å½•ï¼Œè·å–ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
        elif os.path.isdir(p):
            files = sorted(glob.glob(os.path.join(p, '*.*')))  # ä»ç›®å½•ä¸­è·å–æ‰€æœ‰æ–‡ä»¶
        # å¦‚æœè·¯å¾„æ˜¯æ–‡ä»¶ï¼Œç›´æ¥å°†å…¶åŠ å…¥æ–‡ä»¶åˆ—è¡¨
        elif os.path.isfile(p):
            files = [p]  # æ–‡ä»¶è·¯å¾„
        else:
            # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
            raise Exception(f'ERROR: {p} does not exist')

        # å°†æ–‡ä»¶åˆ†ä¸ºå›¾åƒå’Œè§†é¢‘
        images = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]  # å›¾åƒæ–‡ä»¶
        videos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]  # è§†é¢‘æ–‡ä»¶
        ni, nv = len(images), len(videos)  # å›¾åƒå’Œè§†é¢‘çš„æ•°é‡

        self.img_size = img_size  # è®¾ç½®å›¾åƒå¤§å°
        self.stride = stride  # è®¾ç½®æ­¥å¹…
        self.files = images + videos  # åˆå¹¶æ‰€æœ‰æ–‡ä»¶
        self.nf = ni + nv  # æ–‡ä»¶æ€»æ•°é‡
        self.video_flag = [False] * ni + [True] * nv  # æ ‡è®°å“ªäº›æ˜¯å›¾åƒï¼Œå“ªäº›æ˜¯è§†é¢‘
        self.mode = 'image'  # åˆå§‹æ¨¡å¼è®¾ç½®ä¸ºå›¾åƒ
        self.auto = auto  # æ˜¯å¦è‡ªåŠ¨è°ƒæ•´å¤§å°çš„æ ‡å¿—

        # å¦‚æœæœ‰è§†é¢‘æ–‡ä»¶ï¼Œåˆ™åˆå§‹åŒ–ç¬¬ä¸€ä¸ªè§†é¢‘
        if any(videos):
            self.new_video(videos[0])  # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªè§†é¢‘
        else:
            self.cap = None  # æ²¡æœ‰è§†é¢‘æ•è·å¯¹è±¡

        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ–‡ä»¶å­˜åœ¨
        assert self.nf > 0, f'No images or videos found in {p}. ' \
                            f'Supported formats are:\nimages: {IMG_FORMATS}\nvideos: {VID_FORMATS}'

    def __iter__(self):
        # åˆå§‹åŒ–è¿­ä»£è®¡æ•°å™¨
        self.count = 0
        return self  # è¿”å›è‡ªèº«ä»¥æ”¯æŒè¿­ä»£

    def __next__(self):
        # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†å®Œæ‰€æœ‰æ–‡ä»¶
        if self.count == self.nf:
            raise StopIteration  # å¦‚æœæ‰€æœ‰æ–‡ä»¶å·²å¤„ç†ï¼Œåœæ­¢è¿­ä»£

        path = self.files[self.count]  # è·å–å½“å‰æ–‡ä»¶çš„è·¯å¾„

        if self.video_flag[self.count]:
            # å¤„ç†è§†é¢‘æ–‡ä»¶åŠ è½½
            self.mode = 'video'
            ret_val, img0 = self.cap.read()  # ä»è§†é¢‘æ•è·å¯¹è±¡è¯»å–ä¸€å¸§
            if not ret_val:
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œæ›´æ–°è®¡æ•°å¹¶é‡Šæ”¾å½“å‰è§†é¢‘æ•è·å¯¹è±¡
                self.count += 1
                self.cap.release()  # é‡Šæ”¾å½“å‰è§†é¢‘æ•è·å¯¹è±¡
                if self.count == self.nf:  # å¦‚æœå·²è¾¾åˆ°æœ€åä¸€ä¸ªè§†é¢‘
                    raise StopIteration  # åœæ­¢è¿­ä»£
                else:
                    path = self.files[self.count]  # è·å–ä¸‹ä¸€ä¸ªæ–‡ä»¶çš„è·¯å¾„
                    self.new_video(path)  # åŠ è½½ä¸‹ä¸€ä¸ªè§†é¢‘
                    ret_val, img0 = self.cap.read()  # å†æ¬¡å°è¯•è¯»å–å¸§

            self.frame += 1  # æ›´æ–°å½“å‰å¸§è®¡æ•°
            print(f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: ', end='')

        else:
            # å¤„ç†å›¾åƒæ–‡ä»¶åŠ è½½
            self.count += 1  # æ›´æ–°è®¡æ•°å™¨
            img0 = cv2.imread(path)  # ä½¿ç”¨ OpenCV è¯»å–å›¾åƒï¼ˆBGR æ ¼å¼ï¼‰
            assert img0 is not None, 'Image Not Found ' + path  # ç¡®ä¿å›¾åƒæˆåŠŸåŠ è½½
            print(f'image {self.count}/{self.nf} {path}: ', end='')

        # è¿›è¡Œå›¾åƒçš„å¡«å……è°ƒæ•´
        img = letterbox(img0, self.img_size, stride=self.stride, auto=self.auto)[0]

        # å°†å›¾åƒè½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼
        img = img.transpose((2, 0, 1))[::-1]  # å°† HWC è½¬æ¢ä¸º CHWï¼Œå¹¶å°† BGR è½¬æ¢ä¸º RGB
        img = np.ascontiguousarray(img)  # ç¡®ä¿æ•°ç»„æ˜¯è¿ç»­çš„

        # è¿”å›è·¯å¾„ã€å¤„ç†åçš„å›¾åƒã€åŸå§‹å›¾åƒå’Œè§†é¢‘æ•è·å¯¹è±¡
        return path, img, img0, self.cap

    def new_video(self, path):
        # ä¸ºæ–°çš„è§†é¢‘é‡ç½®å¸§è®¡æ•°
        self.frame = 0
        self.cap = cv2.VideoCapture(path)  # åˆå§‹åŒ–è§†é¢‘æ•è·å¯¹è±¡
        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))  # è·å–è§†é¢‘ä¸­çš„æ€»å¸§æ•°

    def __len__(self):
        # è¿”å›æ–‡ä»¶çš„æ•°é‡
        return self.nf  # æ–‡ä»¶æ•°é‡


class LoadWebcam:  # ç”¨äºæ¨ç†
    """
    YOLOv5æœ¬åœ°ç½‘ç»œæ‘„åƒå¤´æ•°æ®åŠ è½½å™¨ï¼Œä¾‹å¦‚ï¼š`python detect.py --source 0`
    """

    def __init__(self, pipe='0', img_size=640, stride=32):
        """
        åˆå§‹åŒ–LoadWebcamå®ä¾‹ã€‚

        Args:
            pipe (str): æ‘„åƒå¤´çš„è¾“å…¥æºï¼Œå¯ä»¥æ˜¯æ•°å­—ï¼ˆæ‘„åƒå¤´IDï¼‰æˆ–å­—ç¬¦ä¸²ï¼ˆè§†é¢‘æ–‡ä»¶è·¯å¾„ï¼‰ã€‚
            img_size (int): è¾“å…¥å›¾åƒçš„å¤§å°ï¼Œé»˜è®¤ä¸º640ã€‚
            stride (int): å›¾åƒå¤„ç†çš„æ­¥å¹…ï¼Œé»˜è®¤ä¸º32ã€‚
        """
        self.img_size = img_size  # è®¾ç½®å›¾åƒå¤§å°
        self.stride = stride  # è®¾ç½®æ­¥å¹…
        self.pipe = eval(pipe) if pipe.isnumeric() else pipe  # è§£æè¾“å…¥æº
        self.cap = cv2.VideoCapture(self.pipe)  # åˆ›å»ºè§†é¢‘æ•æ‰å¯¹è±¡
        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)  # è®¾ç½®ç¼“å†²åŒºå¤§å°

    def __iter__(self):
        """
        åˆå§‹åŒ–è¿­ä»£å™¨ã€‚

        Returns:
            self: è¿”å›è‡ªèº«ä»¥æ”¯æŒè¿­ä»£ã€‚
        """
        self.count = -1  # è®¡æ•°å™¨åˆå§‹åŒ–
        return self

    def __next__(self):
        """
        è·å–ä¸‹ä¸€ä¸ªå¸§çš„æ•°æ®ã€‚

        Returns:
            tuple: åŒ…å«å›¾åƒè·¯å¾„ã€å¤„ç†åçš„å›¾åƒã€åŸå§‹å›¾åƒå’ŒNoneã€‚

        Raises:
            StopIteration: å¦‚æœç”¨æˆ·æŒ‰ä¸‹'q'é”®é€€å‡ºã€‚
        """
        self.count += 1  # é€’å¢è®¡æ•°å™¨
        if cv2.waitKey(1) == ord('q'):  # æ£€æµ‹åˆ°'q'é”®åˆ™é€€å‡º
            self.cap.release()  # é‡Šæ”¾æ‘„åƒå¤´
            cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰OpenCVçª—å£
            raise StopIteration  # å¼•å‘åœæ­¢è¿­ä»£å¼‚å¸¸

        # è¯»å–å¸§
        ret_val, img0 = self.cap.read()  # ä»æ‘„åƒå¤´è¯»å–å›¾åƒ
        img0 = cv2.flip(img0, 1)  # å·¦å³ç¿»è½¬å›¾åƒ

        # æ£€æŸ¥è¯»å–ç»“æœ
        assert ret_val, f'Camera Error {self.pipe}'  # ç¡®ä¿æˆåŠŸè¯»å–
        img_path = 'webcam.jpg'  # å›¾åƒè·¯å¾„
        print(f'webcam {self.count}: ', end='')  # æ‰“å°å½“å‰å¸§è®¡æ•°

        # å¡«å……ç¼©æ”¾
        img = letterbox(img0, self.img_size, stride=self.stride)[0]  # å°†å›¾åƒè°ƒæ•´ä¸ºç›®æ ‡å¤§å°

        # è½¬æ¢å›¾åƒæ ¼å¼
        img = img.transpose((2, 0, 1))[::-1]  # HWCåˆ°CHWï¼Œå¹¶å°†BGRè½¬æ¢ä¸ºRGB
        img = np.ascontiguousarray(img)  # ç¡®ä¿æ•°ç»„æ˜¯è¿ç»­çš„

        return img_path, img, img0, None  # è¿”å›å›¾åƒè·¯å¾„ã€å¤„ç†åçš„å›¾åƒã€åŸå§‹å›¾åƒå’Œå ä½ç¬¦

    def __len__(self):
        """
        è¿”å›æ•°æ®é›†çš„é•¿åº¦ã€‚

        Returns:
            int: ç”±äºè¿™æ˜¯æ— é™åŠ è½½å™¨ï¼Œå› æ­¤è¿”å›0ã€‚
        """
        return 0  # æ— é™åŠ è½½å™¨é•¿åº¦ä¸º0



class LoadStreams:
    # YOLOv5 streamloader, i.e. `python detect.py --source 'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP streams`
    def __init__(self, sources='streams.txt', img_size=640, stride=32, auto=True):
        # åˆå§‹åŒ– LoadStreams ç±»
        self.mode = 'stream'  # è®¾ç½®æ¨¡å¼ä¸ºæµ
        self.img_size = img_size  # è®¾ç½®å›¾åƒå¤§å°
        self.stride = stride  # è®¾ç½®æ­¥å¹…

        # æ£€æŸ¥ sources æ˜¯å¦ä¸ºæ–‡ä»¶ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹
        if os.path.isfile(sources):
            with open(sources, 'r') as f:
                sources = [x.strip() for x in f.read().strip().splitlines() if len(x.strip())]
        else:
            sources = [sources]  # å¦‚æœä¸æ˜¯æ–‡ä»¶ï¼Œç›´æ¥å°†å…¶åŒ…è£…æˆåˆ—è¡¨

        n = len(sources)  # è·å–æºæ•°é‡
        # åˆå§‹åŒ–å›¾åƒã€å¸§ç‡ã€å¸§æ•°å’Œçº¿ç¨‹çš„åˆ—è¡¨
        self.imgs, self.fps, self.frames, self.threads = [None] * n, [0] * n, [0] * n, [None] * n
        self.sources = [clean_str(x) for x in sources]  # æ¸…ç†æºåç§°ä»¥ä¾¿åç»­ä½¿ç”¨
        self.auto = auto  # æ˜¯å¦è‡ªåŠ¨è°ƒæ•´

        for i, s in enumerate(sources):  # éå†æºåˆ—è¡¨
            # å¯åŠ¨çº¿ç¨‹ä»è§†é¢‘æµä¸­è¯»å–å¸§
            print(f'{i + 1}/{n}: {s}... ', end='')  # æ˜¾ç¤ºå½“å‰æºç´¢å¼•å’Œæºåœ°å€
            if 'youtube.com/' in s or 'youtu.be/' in s:  # å¦‚æœæºæ˜¯ YouTube è§†é¢‘
                check_requirements(('pafy', 'youtube_dl'))  # æ£€æŸ¥ä¾èµ–
                import pafy
                s = pafy.new(s).getbest(preftype="mp4").url  # è·å–æœ€ä½³ YouTube URL
            s = eval(s) if s.isnumeric() else s  # å¦‚æœæ˜¯æ•°å­—ï¼Œåˆ™å°†å…¶ä½œä¸ºæœ¬åœ°æ‘„åƒå¤´ç´¢å¼•
            cap = cv2.VideoCapture(s)  # æ‰“å¼€è§†é¢‘æµ
            assert cap.isOpened(), f'Failed to open {s}'  # ç¡®ä¿è§†é¢‘æµå·²æˆåŠŸæ‰“å¼€

            # è·å–è§†é¢‘æµçš„å®½åº¦å’Œé«˜åº¦
            w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            # è·å–å¸§ç‡ï¼Œé»˜è®¤ä¸º 30 FPS
            self.fps[i] = max(cap.get(cv2.CAP_PROP_FPS) % 100, 0) or 30.0
            # è·å–å¸§æ•°ï¼Œé»˜è®¤ä¸ºæ— é™æµ
            self.frames[i] = max(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), 0) or float('inf')

            _, self.imgs[i] = cap.read()  # ç¡®ä¿è¯»å–ç¬¬ä¸€å¸§
            # åˆ›å»ºçº¿ç¨‹ä»¥å¼‚æ­¥æ›´æ–°å¸§
            self.threads[i] = Thread(target=self.update, args=([i, cap, s]), daemon=True)
            print(f" success ({self.frames[i]} frames {w}x{h} at {self.fps[i]:.2f} FPS)")  # è¾“å‡ºæˆåŠŸä¿¡æ¯
            self.threads[i].start()  # å¯åŠ¨çº¿ç¨‹
        print('')  # æ¢è¡Œ

        # æ£€æŸ¥å›¾åƒå½¢çŠ¶çš„ä¸€è‡´æ€§
        s = np.stack([letterbox(x, self.img_size, stride=self.stride, auto=self.auto)[0].shape for x in self.imgs])
        self.rect = np.unique(s, axis=0).shape[0] == 1  # å¦‚æœæ‰€æœ‰å½¢çŠ¶ç›¸ç­‰ï¼Œåˆ™è¿›è¡ŒçŸ©å½¢æ¨ç†
        if not self.rect:
            print('WARNING: Different stream shapes detected. For optimal performance supply similarly-shaped streams.')  # è¾“å‡ºè­¦å‘Š

    def update(self, i, cap, stream):
        # åœ¨å®ˆæŠ¤çº¿ç¨‹ä¸­è¯»å–æµ `i` çš„å¸§
        n, f, read = 0, self.frames[i], 1  # å¸§æ•°ï¼Œå¸§æ•°ç»„ï¼Œæ¯ 'read' å¸§è¿›è¡Œä¸€æ¬¡æ¨ç†
        while cap.isOpened() and n < f:  # å½“è§†é¢‘æµä»ç„¶æ‰“å¼€ä¸”æœªè¾¾åˆ°å¸§æ•°é™åˆ¶æ—¶
            n += 1
            # cap.grab()  # æŠ“å–å¸§
            cap.grab()  # æŠ“å–ä¸‹ä¸€å¸§ï¼Œä¸è¿”å›å›¾åƒ
            if n % read == 0:  # æ¯éš”ä¸€å®šå¸§æ•°è¯»å–ä¸€æ¬¡å›¾åƒ
                success, im = cap.retrieve()  # è·å–å›¾åƒ
                if success:
                    self.imgs[i] = im  # æ›´æ–°å›¾åƒ
                else:
                    print('WARNING: Video stream unresponsive, please check your IP camera connection.')  # è¾“å‡ºè­¦å‘Š
                    self.imgs[i] *= 0  # å¦‚æœè¯»å–å¤±è´¥ï¼Œè®¾ç½®å›¾åƒä¸ºé›¶
                    cap.open(stream)  # é‡æ–°æ‰“å¼€æµï¼Œå¦‚æœä¿¡å·ä¸¢å¤±
            time.sleep(1 / self.fps[i])  # ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿å¸§ç‡

    def __iter__(self):
        self.count = -1  # è®¡æ•°å™¨åˆå§‹åŒ–
        return self  # è¿”å›è¿­ä»£å™¨å¯¹è±¡

    def __next__(self):
        self.count += 1  # è®¡æ•°å™¨é€’å¢
        if not all(x.is_alive() for x in self.threads) or cv2.waitKey(1) == ord('q'):  # å¦‚æœæ‰€æœ‰çº¿ç¨‹éƒ½ä¸å†å­˜æ´»ï¼Œæˆ–è€…æŒ‰ä¸‹ 'q' é”®
            cv2.destroyAllWindows()  # å…³é—­æ‰€æœ‰çª—å£
            raise StopIteration  # åœæ­¢è¿­ä»£

        # Letterbox
        img0 = self.imgs.copy()  # å¤åˆ¶å½“å‰å›¾åƒ
        img = [letterbox(x, self.img_size, stride=self.stride, auto=self.rect and self.auto)[0] for x in img0]  # å¯¹æ¯å¹…å›¾åƒè¿›è¡Œ Letterbox æ“ä½œ

        # Stack
        img = np.stack(img, 0)  # å°†æ‰€æœ‰å›¾åƒå †å ä¸ºä¸€ä¸ªæ‰¹æ¬¡

        # Convert
        img = img[..., ::-1].transpose((0, 3, 1, 2))  # BGR è½¬ RGBï¼ŒBHWC è½¬ BCHW
        img = np.ascontiguousarray(img)  # ç¡®ä¿è¿”å›çš„æ•°ç»„æ˜¯è¿ç»­çš„

        return self.sources, img, img0, None  # è¿”å›æºåœ°å€ã€å¤„ç†åçš„å›¾åƒã€åŸå§‹å›¾åƒå’Œ None

    def __len__(self):
        return len(self.sources)  # è¿”å›æºçš„æ•°é‡ï¼Œä¾¿äºè¿­ä»£



def img2label_paths(img_paths):
    """
    å°†å›¾åƒè·¯å¾„è½¬æ¢ä¸ºæ ‡ç­¾è·¯å¾„ã€‚

    Args:
        img_paths (list): åŒ…å«å›¾åƒæ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ã€‚

    Returns:
        list: å¯¹åº”äºè¾“å…¥å›¾åƒè·¯å¾„çš„æ ‡ç­¾æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
    """
    # å®šä¹‰å›¾åƒå’Œæ ‡ç­¾è·¯å¾„çš„å­å­—ç¬¦ä¸²
    sa, sb = os.sep + 'images' + os.sep, os.sep + 'labels' + os.sep  # '/images/' å’Œ '/labels/' å­å­—ç¬¦ä¸²

    # éå†æ¯ä¸ªå›¾åƒè·¯å¾„ï¼Œå°†å…¶è½¬æ¢ä¸ºæ ‡ç­¾è·¯å¾„
    return [
        sb.join(x.rsplit(sa, 1)).rsplit('.', 1)[0] + '.txt' for x in img_paths
    ]



class LoadImagesAndLabels(Dataset):
    # YOLOv5 çš„è®­ç»ƒ/éªŒè¯æ•°æ®åŠ è½½å™¨ï¼Œç”¨äºåŠ è½½å›¾åƒåŠå…¶æ ‡ç­¾
    cache_version = 0.5  # æ•°æ®é›†æ ‡ç­¾çš„ç¼“å­˜ç‰ˆæœ¬

    def __init__(self, path, img_size=640, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,
                 cache_images=False, single_cls=False, stride=32, pad=0.0, prefix=''):
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨çš„å‚æ•°
        self.img_size = img_size  # å›¾åƒå¤§å°
        self.augment = augment  # æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼º
        self.hyp = hyp  # è¶…å‚æ•°
        self.image_weights = image_weights  # æ˜¯å¦ä½¿ç”¨å›¾åƒæƒé‡
        self.rect = False if image_weights else rect  # å¦‚æœä½¿ç”¨å›¾åƒæƒé‡ï¼Œåˆ™ä¸ä½¿ç”¨çŸ©å½¢è®­ç»ƒ
        self.mosaic = self.augment and not self.rect  # æ˜¯å¦ä½¿ç”¨é©¬èµ›å…‹å¢å¼ºï¼ˆä»…åœ¨è®­ç»ƒæ—¶ï¼‰
        self.mosaic_border = [-img_size // 2, -img_size // 2]  # é©¬èµ›å…‹å¢å¼ºçš„è¾¹ç•Œ
        self.stride = stride  # ç½‘ç»œçš„æ­¥å¹…
        self.path = path  # å›¾åƒè·¯å¾„
        self.albumentations = Albumentations() if augment else None  # åˆå§‹åŒ–æ•°æ®å¢å¼ºå·¥å…·

        try:
            f = []  # å›¾åƒæ–‡ä»¶åˆ—è¡¨
            for p in path if isinstance(path, list) else [path]:
                p = Path(p)  # å¤„ç†è·¯å¾„ï¼Œé€‚åº”ä¸åŒæ“ä½œç³»ç»Ÿ
                if p.is_dir():  # å¦‚æœæ˜¯ç›®å½•
                    f += glob.glob(str(p / '**' / '*.*'), recursive=True)  # é€’å½’æŸ¥æ‰¾å›¾åƒæ–‡ä»¶
                elif p.is_file():  # å¦‚æœæ˜¯æ–‡ä»¶
                    with open(p, 'r') as t:
                        t = t.read().strip().splitlines()  # è¯»å–æ–‡ä»¶å†…å®¹
                        parent = str(p.parent) + os.sep  # çˆ¶ç›®å½•è·¯å¾„
                        f += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # æ›´æ–°è·¯å¾„
                else:
                    raise Exception(f'{prefix}{p} does not exist')  # æŠ¥é”™ï¼šè·¯å¾„ä¸å­˜åœ¨
            # ç­›é€‰å¹¶æ’åºå›¾åƒæ–‡ä»¶
            self.img_files = sorted([x.replace('/', os.sep) for x in f if x.split('.')[-1].lower() in IMG_FORMATS])
            assert self.img_files, f'{prefix}No images found'  # ç¡®ä¿æ‰¾åˆ°å›¾åƒ
        except Exception as e:
            raise Exception(f'{prefix}Error loading data from {path}: {e}\nSee {HELP_URL}')  # æŠ¥é”™ï¼šåŠ è½½æ•°æ®æ—¶å‡ºé”™

        # æ£€æŸ¥ç¼“å­˜
        self.label_files = img2label_paths(self.img_files)  # è·å–æ ‡ç­¾æ–‡ä»¶è·¯å¾„
        cache_path = (p if p.is_file() else Path(self.label_files[0]).parent).with_suffix('.cache')  # è®¾ç½®ç¼“å­˜è·¯å¾„
        try:
            # å°è¯•åŠ è½½ç¼“å­˜
            cache, exists = np.load(cache_path, allow_pickle=True).item(), True  # åŠ è½½ç¼“å­˜å­—å…¸
            assert cache['version'] == self.cache_version  # ç¡®ä¿ç¼“å­˜ç‰ˆæœ¬ä¸€è‡´
            assert cache['hash'] == get_hash(self.label_files + self.img_files)  # ç¡®ä¿ç¼“å­˜å“ˆå¸Œä¸€è‡´
        except:
            # å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ–°çš„ç¼“å­˜
            cache, exists = self.cache_labels(cache_path, prefix), False  # åˆ›å»ºæ–°çš„ç¼“å­˜

        # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
        nf, nm, ne, nc, n = cache.pop('results')  # æå–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        if exists:
            d = f"Scanning '{cache_path}' images and labels... {nf} found, {nm} missing, {ne} empty, {nc} corrupted"
            tqdm(None, desc=prefix + d, total=n, initial=n)  # æ˜¾ç¤ºç¼“å­˜ç»“æœ
            if cache['msgs']:
                logging.info('\n'.join(cache['msgs']))  # æ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
        assert nf > 0 or not augment, f'{prefix}No labels in {cache_path}. Can not train without labels. See {HELP_URL}'  # ç¡®ä¿æ‰¾åˆ°æ ‡ç­¾

        # è¯»å–ç¼“å­˜
        [cache.pop(k) for k in ('hash', 'version', 'msgs')]  # ç§»é™¤ä¸éœ€è¦çš„é¡¹
        labels, shapes, self.segments = zip(*cache.values())  # è§£å‹æ ‡ç­¾ã€å½¢çŠ¶å’Œåˆ†æ®µä¿¡æ¯
        self.labels = list(labels)  # æ ‡ç­¾
        self.shapes = np.array(shapes, dtype=np.float64)  # å›¾åƒå½¢çŠ¶
        self.img_files = list(cache.keys())  # æ›´æ–°å›¾åƒæ–‡ä»¶åˆ—è¡¨
        self.label_files = img2label_paths(cache.keys())  # æ›´æ–°æ ‡ç­¾æ–‡ä»¶åˆ—è¡¨
        if single_cls:
            for x in self.labels:
                x[:, 0] = 0  # å¦‚æœæ˜¯å•ç±»ä»»åŠ¡ï¼Œåˆ™å°†æ‰€æœ‰æ ‡ç­¾çš„ç±»ç¼–å·è®¾ç½®ä¸º0

        n = len(shapes)  # å›¾åƒæ•°é‡
        bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # æ‰¹æ¬¡ç´¢å¼•
        nb = bi[-1] + 1  # æ‰¹æ¬¡æ•°é‡
        self.batch = bi  # å›¾åƒçš„æ‰¹æ¬¡ç´¢å¼•
        self.n = n  # æ€»å›¾åƒæ•°é‡
        self.indices = range(n)  # ç´¢å¼•èŒƒå›´

        # çŸ©å½¢è®­ç»ƒ
        if self.rect:
            # æ ¹æ®å®½é«˜æ¯”æ’åº
            s = self.shapes  # å›¾åƒçš„å®½é«˜
            ar = s[:, 1] / s[:, 0]  # è®¡ç®—å®½é«˜æ¯”
            irect = ar.argsort()  # è·å–æ’åºç´¢å¼•
            self.img_files = [self.img_files[i] for i in irect]  # æŒ‰å®½é«˜æ¯”æ’åºå›¾åƒæ–‡ä»¶
            self.label_files = [self.label_files[i] for i in irect]  # æŒ‰å®½é«˜æ¯”æ’åºæ ‡ç­¾æ–‡ä»¶
            self.labels = [self.labels[i] for i in irect]  # æŒ‰å®½é«˜æ¯”æ’åºæ ‡ç­¾
            self.shapes = s[irect]  # æ›´æ–°å›¾åƒå½¢çŠ¶
            ar = ar[irect]  # æ›´æ–°å®½é«˜æ¯”

            # è®¾ç½®è®­ç»ƒå›¾åƒçš„å½¢çŠ¶
            shapes = [[1, 1]] * nb  # åˆå§‹åŒ–å½¢çŠ¶åˆ—è¡¨
            for i in range(nb):
                ari = ar[bi == i]  # è·å–å½“å‰æ‰¹æ¬¡çš„å®½é«˜æ¯”
                mini, maxi = ari.min(), ari.max()  # è·å–æœ€å°å’Œæœ€å¤§å®½é«˜æ¯”
                if maxi < 1:
                    shapes[i] = [maxi, 1]  # å¦‚æœæœ€å¤§å®½é«˜æ¯”å°äº1ï¼Œè®¾ç½®å½¢çŠ¶ä¸º [maxi, 1]
                elif mini > 1:
                    shapes[i] = [1, 1 / mini]  # å¦‚æœæœ€å°å®½é«˜æ¯”å¤§äº1ï¼Œè®¾ç½®å½¢çŠ¶ä¸º [1, 1/mini]

            # è®¡ç®—æ‰¹æ¬¡å½¢çŠ¶ï¼Œå‘ä¸Šå–æ•´å¹¶è¿›è¡Œæ­¥å¹…è°ƒæ•´
            self.batch_shapes = np.ceil(np.array(shapes) * img_size / stride + pad).astype(np.int) * stride

        # å°†å›¾åƒç¼“å­˜åˆ°å†…å­˜ä»¥åŠ é€Ÿè®­ç»ƒï¼ˆè­¦å‘Šï¼šå¤§å‹æ•°æ®é›†å¯èƒ½ä¼šè¶…å‡ºç³»ç»Ÿå†…å­˜ï¼‰
        self.imgs, self.img_npy = [None] * n, [None] * n  # åˆå§‹åŒ–å›¾åƒå’Œç¼“å­˜è·¯å¾„
        if cache_images:
            if cache_images == 'disk':
                # å¦‚æœé€‰æ‹©å°†å›¾åƒç¼“å­˜åˆ°ç£ç›˜
                self.im_cache_dir = Path(Path(self.img_files[0]).parent.as_posix() + '_npy')  # ç¼“å­˜ç›®å½•
                self.img_npy = [self.im_cache_dir / Path(f).with_suffix('.npy').name for f in self.img_files]  # ç¼“å­˜æ–‡ä»¶è·¯å¾„
                self.im_cache_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç¼“å­˜ç›®å½•
            gb = 0  # ç¼“å­˜å›¾åƒçš„å¤§å°ï¼ˆä»¥GBä¸ºå•ä½ï¼‰
            self.img_hw0, self.img_hw = [None] * n, [None] * n  # åˆå§‹åŒ–åŸå§‹å’Œè°ƒæ•´åå›¾åƒå¤§å°
            results = ThreadPool(NUM_THREADS).imap(lambda x: load_image(*x), zip(repeat(self), range(n)))  # å¤šçº¿ç¨‹åŠ è½½å›¾åƒ
            pbar = tqdm(enumerate(results), total=n)  # åˆå§‹åŒ–è¿›åº¦æ¡
            for i, x in pbar:
                if cache_images == 'disk':
                    if not self.img_npy[i].exists():
                        np.save(self.img_npy[i].as_posix(), x[0])  # ä¿å­˜ç¼“å­˜å›¾åƒ
                    gb += self.img_npy[i].stat().st_size  # æ›´æ–°ç¼“å­˜å¤§å°
                else:
                    self.imgs[i], self.img_hw0[i], self.img_hw[i] = x  # åŠ è½½å›¾åƒåŠå…¶å¤§å°
                    gb += self.imgs[i].nbytes  # æ›´æ–°ç¼“å­˜å¤§å°
                pbar.desc = f'{prefix}Caching images ({gb / 1E9:.1f}GB {cache_images})'  # æ›´æ–°è¿›åº¦æè¿°
            pbar.close()  # å…³é—­è¿›åº¦æ¡

    def cache_labels(self, path=Path('./labels.cache'), prefix=''):
        # ç¼“å­˜æ•°æ®é›†æ ‡ç­¾ï¼Œæ£€æŸ¥å›¾åƒå¹¶è¯»å–å½¢çŠ¶
        x = {}  # åˆå§‹åŒ–å­—å…¸ç”¨äºå­˜å‚¨å›¾åƒã€æ ‡ç­¾ã€å½¢çŠ¶å’Œæ®µè½ä¿¡æ¯
        nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # ç»Ÿè®¡ç¼ºå¤±ã€æ‰¾åˆ°ã€ç©ºã€æŸåçš„æ•°é‡å’Œæ¶ˆæ¯
        desc = f"{prefix}Scanning '{path.parent / path.stem}' images and labels..."  # æè¿°ä¿¡æ¯
        with Pool(NUM_THREADS) as pool:  # åˆ›å»ºçº¿ç¨‹æ± 
            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå›¾åƒå’Œæ ‡ç­¾éªŒè¯çš„è¿›åº¦
            pbar = tqdm(pool.imap(verify_image_label, zip(self.img_files, self.label_files, repeat(prefix))),
                        desc=desc, total=len(self.img_files))
            for im_file, l, shape, segments, nm_f, nf_f, ne_f, nc_f, msg in pbar:
                # æ›´æ–°è®¡æ•°å™¨
                nm += nm_f  # æ›´æ–°ç¼ºå¤±çš„æ ‡ç­¾æ•°é‡
                nf += nf_f  # æ›´æ–°æ‰¾åˆ°çš„æ ‡ç­¾æ•°é‡
                ne += ne_f  # æ›´æ–°ç©ºæ ‡ç­¾æ•°é‡
                nc += nc_f  # æ›´æ–°æŸåæ ‡ç­¾æ•°é‡
                if im_file:  # å¦‚æœæ‰¾åˆ°äº†å›¾åƒæ–‡ä»¶
                    x[im_file] = [l, shape, segments]  # å°†å›¾åƒæ–‡ä»¶ã€æ ‡ç­¾ã€å½¢çŠ¶å’Œæ®µè½å­˜å‚¨åœ¨å­—å…¸ä¸­
                if msg:  # å¦‚æœæœ‰æ¶ˆæ¯
                    msgs.append(msg)  # æ”¶é›†æ¶ˆæ¯
                # æ›´æ–°è¿›åº¦æ¡æè¿°
                pbar.desc = f"{desc}{nf} found, {nm} missing, {ne} empty, {nc} corrupted"

        pbar.close()  # å…³é—­è¿›åº¦æ¡
        if msgs:  # å¦‚æœæœ‰è­¦å‘Šæ¶ˆæ¯
            logging.info('\n'.join(msgs))  # è®°å½•è­¦å‘Šæ¶ˆæ¯
        if nf == 0:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ‡ç­¾
            logging.info(f'{prefix}WARNING: No labels found in {path}. See {HELP_URL}')  # å‘å‡ºè­¦å‘Š

        # ç”Ÿæˆç¼“å­˜æ•°æ®
        x['hash'] = get_hash(self.label_files + self.img_files)  # è®¡ç®—å“ˆå¸Œå€¼
        x['results'] = nf, nm, ne, nc, len(self.img_files)  # ä¿å­˜ç»Ÿè®¡ç»“æœ
        x['msgs'] = msgs  # ä¿å­˜è­¦å‘Šä¿¡æ¯
        x['version'] = self.cache_version  # ç¼“å­˜ç‰ˆæœ¬

        try:
            np.save(path, x)  # ä¿å­˜ç¼“å­˜ä»¥ä¾¿ä¸‹æ¬¡ä½¿ç”¨
            path.with_suffix('.cache.npy').rename(path)  # ç§»é™¤ .npy åç¼€å¹¶é‡å‘½å
            logging.info(f'{prefix}New cache created: {path}')  # è®°å½•æ–°ç¼“å­˜åˆ›å»ºçš„ä¿¡æ¯
        except Exception as e:
            logging.info(f'{prefix}WARNING: Cache directory {path.parent} is not writeable: {e}')  # è®°å½•ç›®å½•ä¸å¯å†™çš„è­¦å‘Š

        return x  # è¿”å›ç¼“å­˜æ•°æ®

    def __len__(self):
        # è¿”å›å›¾åƒæ–‡ä»¶çš„æ•°é‡
        return len(self.img_files)

    def __getitem__(self, index):
        # æ ¹æ®ç´¢å¼•è·å–å›¾åƒå’Œæ ‡ç­¾
        index = self.indices[index]  # çº¿æ€§ã€æ‰“ä¹±æˆ–æ ¹æ®å›¾åƒæƒé‡è·å–ç´¢å¼•

        hyp = self.hyp  # è¶…å‚æ•°
        mosaic = self.mosaic and random.random() < hyp['mosaic']  # å†³å®šæ˜¯å¦ä½¿ç”¨é©¬èµ›å…‹å¢å¼º
        if mosaic:
            # åŠ è½½é©¬èµ›å…‹å›¾åƒ
            img, labels = load_mosaic(self, index)  # åŠ è½½é©¬èµ›å…‹å›¾åƒå’Œæ ‡ç­¾
            shapes = None  # ä¸å­˜å‚¨å½¢çŠ¶ä¿¡æ¯

            # MixUp å¢å¼º
            if random.random() < hyp['mixup']:
                img, labels = mixup(img, labels, *load_mosaic(self, random.randint(0, self.n - 1)))

        else:
            # åŠ è½½å•ä¸ªå›¾åƒ
            img, (h0, w0), (h, w) = load_image(self, index)  # åŠ è½½å›¾åƒåŠå…¶åŸå§‹å’Œè°ƒæ•´åçš„å°ºå¯¸

            # ä¿¡çº¸æ¡†å¤„ç†
            shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size  # æœ€ç»ˆä¿¡çº¸æ¡†å½¢çŠ¶
            img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)  # è°ƒæ•´å›¾åƒå½¢çŠ¶
            shapes = (h0, w0), ((h / h0, w / w0), pad)  # ä¸º COCO mAP é‡æ ‡å®šå­˜å‚¨å½¢çŠ¶ä¿¡æ¯

            labels = self.labels[index].copy()  # è·å–æ ‡ç­¾
            if labels.size:  # å¦‚æœæœ‰æ ‡ç­¾ï¼Œå°†æ ‡å‡†åŒ–çš„ xywh è½¬æ¢ä¸ºåƒç´ çš„ xyxy æ ¼å¼
                labels[:, 1:] = xywhn2xyxy(labels[:, 1:], ratio[0] * w, ratio[1] * h, padw=pad[0], padh=pad[1])

            if self.augment:  # å¦‚æœéœ€è¦å¢å¼º
                img, labels = random_perspective(img, labels,  # éšæœºé€è§†å˜æ¢
                                                 degrees=hyp['degrees'],
                                                 translate=hyp['translate'],
                                                 scale=hyp['scale'],
                                                 shear=hyp['shear'],
                                                 perspective=hyp['perspective'])

        nl = len(labels)  # æ ‡ç­¾æ•°é‡
        if nl:
            labels[:, 1:5] = xyxy2xywhn(labels[:, 1:5], w=img.shape[1], h=img.shape[0], clip=True,
                                        eps=1E-3)  # è½¬æ¢æ ‡ç­¾ä¸ºæ ‡å‡†æ ¼å¼

        if self.augment:
            # ä½¿ç”¨ Albumentations è¿›è¡Œæ•°æ®å¢å¼º
            img, labels = self.albumentations(img, labels)
            nl = len(labels)  # æ›´æ–°æ ‡ç­¾æ•°é‡

            # HSV é¢œè‰²ç©ºé—´å¢å¼º
            augment_hsv(img, hgain=hyp['hsv_h'], sgain=hyp['hsv_s'], vgain=hyp['hsv_v'])

            # ä¸Šä¸‹ç¿»è½¬
            if random.random() < hyp['flipud']:
                img = np.flipud(img)  # ç¿»è½¬å›¾åƒ
                if nl:
                    labels[:, 2] = 1 - labels[:, 2]  # æ›´æ–°æ ‡ç­¾åæ ‡

            # å·¦å³ç¿»è½¬
            if random.random() < hyp['fliplr']:
                img = np.fliplr(img)  # ç¿»è½¬å›¾åƒ
                if nl:
                    labels[:, 1] = 1 - labels[:, 1]  # æ›´æ–°æ ‡ç­¾åæ ‡

            # Cutouts å¢å¼ºï¼ˆå¯é€‰ï¼‰
            # labels = cutout(img, labels, p=0.5)

        # åˆ›å»ºè¾“å‡ºæ ‡ç­¾
        labels_out = torch.zeros((nl, 6))  # åˆå§‹åŒ–æ ‡ç­¾è¾“å‡º
        if nl:
            labels_out[:, 1:] = torch.from_numpy(labels)  # å°†æ ‡ç­¾è½¬ä¸º PyTorch å¼ é‡

        # è½¬æ¢å›¾åƒæ ¼å¼
        img = img.transpose((2, 0, 1))[::-1]  # HWC è½¬ä¸º CHWï¼ŒåŒæ—¶ä» BGR è½¬ä¸º RGB
        img = np.ascontiguousarray(img)  # ç¡®ä¿å›¾åƒæ˜¯è¿ç»­çš„å†…å­˜å—

        return torch.from_numpy(img), labels_out, self.img_files[index], shapes  # è¿”å›å›¾åƒã€æ ‡ç­¾ã€æ–‡ä»¶åå’Œå½¢çŠ¶ä¿¡æ¯

    @staticmethod
    def collate_fn(batch):
        # ä»æ‰¹æ¬¡ä¸­æå–å›¾åƒã€æ ‡ç­¾ã€è·¯å¾„å’Œå½¢çŠ¶
        img, label, path, shapes = zip(*batch)  # è¿›è¡Œè½¬ç½®
        for i, l in enumerate(label):
            l[:, 0] = i  # ä¸ºæ¯ä¸ªæ ‡ç­¾æ·»åŠ ç›®æ ‡å›¾åƒç´¢å¼•ï¼Œä¾¿äºåç»­å¤„ç†
        return torch.stack(img, 0), torch.cat(label, 0), path, shapes  # è¿”å›å †å çš„å›¾åƒã€åˆå¹¶çš„æ ‡ç­¾ã€è·¯å¾„å’Œå½¢çŠ¶

    @staticmethod
    def collate_fn4(batch):
        # ä»æ‰¹æ¬¡ä¸­æå–å›¾åƒã€æ ‡ç­¾ã€è·¯å¾„å’Œå½¢çŠ¶
        img, label, path, shapes = zip(*batch)  # è¿›è¡Œè½¬ç½®
        n = len(shapes) // 4  # è®¡ç®—æ¯ç»„å›¾åƒçš„æ•°é‡
        img4, label4, path4, shapes4 = [], [], path[:n], shapes[:n]  # åˆå§‹åŒ–è¾“å‡ºåˆ—è¡¨

        # å®šä¹‰ç”¨äºå›¾åƒå¤„ç†çš„å¼ é‡
        ho = torch.tensor([[0., 0, 0, 1, 0, 0]])  # åç§»é‡
        wo = torch.tensor([[0., 0, 1, 0, 0, 0]])  # åç§»é‡
        s = torch.tensor([[1, 1, .5, .5, .5, .5]])  # ç¼©æ”¾å› å­
        for i in range(n):  # éå†æ¯ç»„å›¾åƒ
            i *= 4  # è®¡ç®—ç´¢å¼•
            if random.random() < 0.5:  # éšæœºå†³å®šå›¾åƒå¤„ç†æ–¹å¼
                # é€šè¿‡æ’å€¼æ‰©å¤§å›¾åƒ
                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2., mode='bilinear', align_corners=False)[
                    0].type(img[i].type())
                l = label[i]  # ç›´æ¥ä½¿ç”¨æ ‡ç­¾
            else:
                # æ‹¼æ¥å››ä¸ªå›¾åƒ
                im = torch.cat((torch.cat((img[i], img[i + 1]), 1), torch.cat((img[i + 2], img[i + 3]), 1)), 2)
                # æ‹¼æ¥å¹¶è°ƒæ•´æ ‡ç­¾
                l = torch.cat((label[i], label[i + 1] + ho, label[i + 2] + wo, label[i + 3] + ho + wo), 0) * s
            img4.append(im)  # æ·»åŠ å¤„ç†åçš„å›¾åƒ
            label4.append(l)  # æ·»åŠ å¤„ç†åçš„æ ‡ç­¾

        for i, l in enumerate(label4):
            l[:, 0] = i  # ä¸ºæ¯ä¸ªæ ‡ç­¾æ·»åŠ ç›®æ ‡å›¾åƒç´¢å¼•

        return torch.stack(img4, 0), torch.cat(label4, 0), path4, shapes4  # è¿”å›å †å çš„å›¾åƒã€åˆå¹¶çš„æ ‡ç­¾ã€è·¯å¾„å’Œå½¢çŠ¶


# Ancillary functions --------------------------------------------------------------------------------------------------
def load_image(self, i):
    # ä»æ•°æ®é›†ä¸­åŠ è½½ç´¢å¼• 'i' çš„å›¾åƒï¼Œè¿”å›å›¾åƒã€åŸå§‹é«˜å®½å’Œè°ƒæ•´åçš„é«˜å®½
    im = self.imgs[i]  # å°è¯•ä»ç¼“å­˜ä¸­è·å–å›¾åƒ
    if im is None:  # å¦‚æœå›¾åƒæ²¡æœ‰è¢«ç¼“å­˜åˆ°å†…å­˜ä¸­
        npy = self.img_npy[i]  # è·å–å¯¹åº”çš„ .npy æ–‡ä»¶è·¯å¾„
        if npy and npy.exists():  # å¦‚æœ .npy æ–‡ä»¶å­˜åœ¨
            im = np.load(npy)  # åŠ è½½ .npy æ–‡ä»¶ä¸­çš„å›¾åƒæ•°æ®
        else:  # å¦åˆ™è¯»å–å›¾åƒæ–‡ä»¶
            path = self.img_files[i]  # è·å–å›¾åƒæ–‡ä»¶çš„è·¯å¾„
            im = cv2.imread(path)  # ä½¿ç”¨ OpenCV è¯»å–å›¾åƒ (BGR æ ¼å¼)
            assert im is not None, 'Image Not Found ' + path  # ç¡®ä¿å›¾åƒè¢«æ­£ç¡®åŠ è½½
        h0, w0 = im.shape[:2]  # è·å–åŸå§‹å›¾åƒçš„é«˜å’Œå®½
        r = self.img_size / max(h0, w0)  # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        if r != 1:  # å¦‚æœå›¾åƒå¤§å°ä¸ç­‰
            # æ ¹æ®ç¼©æ”¾æ¯”ä¾‹è°ƒæ•´å›¾åƒå¤§å°
            im = cv2.resize(im, (int(w0 * r), int(h0 * r)),
                            interpolation=cv2.INTER_AREA if r < 1 and not self.augment else cv2.INTER_LINEAR)
        return im, (h0, w0), im.shape[:2]  # è¿”å›è°ƒæ•´åçš„å›¾åƒã€åŸå§‹é«˜å®½å’Œè°ƒæ•´åçš„é«˜å®½
    else:
        # å¦‚æœå›¾åƒå·²ç¼“å­˜ï¼Œç›´æ¥è¿”å›ç¼“å­˜çš„å›¾åƒå’Œé«˜å®½ä¿¡æ¯
        return self.imgs[i], self.img_hw0[i], self.img_hw[i]  # è¿”å›å›¾åƒã€åŸå§‹é«˜å®½å’Œè°ƒæ•´åçš„é«˜å®½


def load_mosaic(self, index):
    # YOLOv5 4-mosaic åŠ è½½å™¨ã€‚åŠ è½½1å¼ å›¾åƒå’Œ3å¼ éšæœºå›¾åƒå½¢æˆä¸€ä¸ª4å›¾åƒæ‹¼æ¥
    labels4, segments4 = [], []  # ç”¨äºå­˜å‚¨æ‹¼æ¥åçš„æ ‡ç­¾å’Œåˆ†æ®µ
    s = self.img_size  # å®šä¹‰æ‹¼æ¥å›¾åƒçš„å¤§å°
    # éšæœºç”Ÿæˆæ‹¼æ¥ä¸­å¿ƒç‚¹çš„åæ ‡
    yc, xc = [int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border]
    # éšæœºé€‰æ‹©3ä¸ªé¢å¤–çš„å›¾åƒç´¢å¼•
    indices = [index] + random.choices(self.indices, k=3)
    random.shuffle(indices)  # éšæœºæ‰“ä¹±ç´¢å¼•é¡ºåº

    for i, index in enumerate(indices):
        # åŠ è½½å›¾åƒ
        img, _, (h, w) = load_image(self, index)

        # æ ¹æ®ç´¢å¼•æ”¾ç½®å›¾åƒ
        if i == 0:  # å·¦ä¸Šè§’
            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # åˆ›å»ºä¸€ä¸ªåŸºäº114çš„ç©ºç™½æ‹¼æ¥å›¾åƒ
            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # å¤§å›¾åƒçš„åæ ‡
            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # å°å›¾åƒçš„åæ ‡
        elif i == 1:  # å³ä¸Šè§’
            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc
            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h
        elif i == 2:  # å·¦ä¸‹è§’
            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)
            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)
        elif i == 3:  # å³ä¸‹è§’
            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)
            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)

        # å°†å°å›¾åƒæ”¾ç½®åˆ°æ‹¼æ¥å›¾åƒçš„æŒ‡å®šä½ç½®
        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]
        padw = x1a - x1b  # æ°´å¹³åç§»é‡
        padh = y1a - y1b  # å‚ç›´åç§»é‡

        # å¤„ç†æ ‡ç­¾
        labels, segments = self.labels[index].copy(), self.segments[index].copy()
        if labels.size:  # å¦‚æœæœ‰æ ‡ç­¾
            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padw, padh)  # å°†å½’ä¸€åŒ–çš„xywhè½¬æ¢ä¸ºåƒç´ xyxyæ ¼å¼
            segments = [xyn2xy(x, w, h, padw, padh) for x in segments]  # è½¬æ¢åˆ†æ®µåæ ‡
        labels4.append(labels)  # æ·»åŠ æ ‡ç­¾
        segments4.extend(segments)  # æ·»åŠ åˆ†æ®µ

    # åˆå¹¶å’Œè£å‰ªæ ‡ç­¾
    labels4 = np.concatenate(labels4, 0)  # åˆå¹¶æ‰€æœ‰æ ‡ç­¾
    for x in (labels4[:, 1:], *segments4):
        np.clip(x, 0, 2 * s, out=x)  # è£å‰ªåæ ‡ï¼Œä»¥å…è¶…å‡ºèŒƒå›´

    # æ•°æ®å¢å¼º
    img4, labels4, segments4 = copy_paste(img4, labels4, segments4, p=self.hyp['copy_paste'])
    img4, labels4 = random_perspective(img4, labels4, segments4,
                                       degrees=self.hyp['degrees'],
                                       translate=self.hyp['translate'],
                                       scale=self.hyp['scale'],
                                       shear=self.hyp['shear'],
                                       perspective=self.hyp['perspective'],
                                       border=self.mosaic_border)  # éšæœºé€è§†å˜æ¢

    return img4, labels4  # è¿”å›æ‹¼æ¥å›¾åƒå’Œæ ‡ç­¾



def load_mosaic9(self, index):
    # YOLOv5 9-mosaic åŠ è½½å™¨ã€‚åŠ è½½1å¼ å›¾åƒå’Œ8å¼ éšæœºå›¾åƒå½¢æˆä¸€ä¸ª9å›¾åƒæ‹¼æ¥
    labels9, segments9 = [], []  # ç”¨äºå­˜å‚¨æ‹¼æ¥åçš„æ ‡ç­¾å’Œåˆ†æ®µ
    s = self.img_size  # å®šä¹‰æ‹¼æ¥å›¾åƒçš„å¤§å°
    # éšæœºé€‰æ‹©8ä¸ªé¢å¤–çš„å›¾åƒç´¢å¼•
    indices = [index] + random.choices(self.indices, k=8)
    random.shuffle(indices)  # éšæœºæ‰“ä¹±ç´¢å¼•é¡ºåº

    for i, index in enumerate(indices):
        # åŠ è½½å›¾åƒ
        img, _, (h, w) = load_image(self, index)

        # æ ¹æ®ç´¢å¼•æ”¾ç½®å›¾åƒ
        if i == 0:  # ä¸­å¿ƒ
            img9 = np.full((s * 3, s * 3, img.shape[2]), 114, dtype=np.uint8)  # åˆ›å»ºä¸€ä¸ªåŸºäº114çš„ç©ºç™½æ‹¼æ¥å›¾åƒ
            h0, w0 = h, w
            c = s, s, s + w, s + h  # (xmin, ymin, xmax, ymax) åæ ‡
        elif i == 1:  # é¡¶éƒ¨
            c = s, s - h, s + w, s
        elif i == 2:  # å³ä¸Šè§’
            c = s + wp, s - h, s + wp + w, s
        elif i == 3:  # å³ä¾§
            c = s + w0, s, s + w0 + w, s + h
        elif i == 4:  # å³ä¸‹è§’
            c = s + w0, s + hp, s + w0 + w, s + hp + h
        elif i == 5:  # åº•éƒ¨
            c = s + w0 - w, s + h0, s + w0, s + h0 + h
        elif i == 6:  # å·¦ä¸‹è§’
            c = s + w0 - wp - w, s + h0, s + w0 - wp, s + h0 + h
        elif i == 7:  # å·¦ä¾§
            c = s - w, s + h0 - h, s, s + h0
        elif i == 8:  # å·¦ä¸Šè§’
            c = s - w, s + h0 - hp - h, s, s + h0 - hp

        padx, pady = c[:2]  # è®°å½•åç§»é‡
        x1, y1, x2, y2 = [max(x, 0) for x in c]  # è®¡ç®—åæ ‡ï¼Œç¡®ä¿ä¸è¶…å‡ºè¾¹ç•Œ

        # å¤„ç†æ ‡ç­¾
        labels, segments = self.labels[index].copy(), self.segments[index].copy()
        if labels.size:  # å¦‚æœæœ‰æ ‡ç­¾
            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padx, pady)  # å°†å½’ä¸€åŒ–çš„xywhè½¬æ¢ä¸ºåƒç´ xyxyæ ¼å¼
            segments = [xyn2xy(x, w, h, padx, pady) for x in segments]  # è½¬æ¢åˆ†æ®µåæ ‡
        labels9.append(labels)  # æ·»åŠ æ ‡ç­¾
        segments9.extend(segments)  # æ·»åŠ åˆ†æ®µ

        # å°†å°å›¾åƒæ”¾ç½®åˆ°æ‹¼æ¥å›¾åƒçš„æŒ‡å®šä½ç½®
        img9[y1:y2, x1:x2] = img[y1 - pady:, x1 - padx:]  # img9[ymin:ymax, xmin:xmax]
        hp, wp = h, w  # è®°å½•å‰ä¸€å¼ å›¾åƒçš„é«˜åº¦å’Œå®½åº¦

    # éšæœºåç§»ä¸­å¿ƒç‚¹
    yc, xc = [int(random.uniform(0, s)) for _ in self.mosaic_border]  # éšæœºç”Ÿæˆæ‹¼æ¥ä¸­å¿ƒç‚¹çš„åæ ‡
    img9 = img9[yc:yc + 2 * s, xc:xc + 2 * s]  # ä»¥ä¸­å¿ƒç‚¹è£å‰ªæ‹¼æ¥å›¾åƒ

    # åˆå¹¶å’Œè£å‰ªæ ‡ç­¾
    labels9 = np.concatenate(labels9, 0)  # åˆå¹¶æ‰€æœ‰æ ‡ç­¾
    labels9[:, [1, 3]] -= xc  # æ›´æ–°æ ‡ç­¾çš„xåæ ‡
    labels9[:, [2, 4]] -= yc  # æ›´æ–°æ ‡ç­¾çš„yåæ ‡
    c = np.array([xc, yc])  # è®°å½•ä¸­å¿ƒç‚¹
    segments9 = [x - c for x in segments9]  # æ›´æ–°åˆ†æ®µåæ ‡

    for x in (labels9[:, 1:], *segments9):
        np.clip(x, 0, 2 * s, out=x)  # è£å‰ªåæ ‡ï¼Œä»¥å…è¶…å‡ºèŒƒå›´

    # æ•°æ®å¢å¼º
    img9, labels9 = random_perspective(img9, labels9, segments9,
                                       degrees=self.hyp['degrees'],
                                       translate=self.hyp['translate'],
                                       scale=self.hyp['scale'],
                                       shear=self.hyp['shear'],
                                       perspective=self.hyp['perspective'],
                                       border=self.mosaic_border)  # éšæœºé€è§†å˜æ¢

    return img9, labels9  # è¿”å›æ‹¼æ¥å›¾åƒå’Œæ ‡ç­¾


def create_folder(path='./new'):
    # åˆ›å»ºæ–‡ä»¶å¤¹å‡½æ•°
    # å‚æ•°:
    # path (str): è¦åˆ›å»ºçš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé»˜è®¤ä¸º './new'

    if os.path.exists(path):
        # æ£€æŸ¥æŒ‡å®šè·¯å¾„æ˜¯å¦å·²å­˜åœ¨
        shutil.rmtree(path)  # å¦‚æœå­˜åœ¨ï¼Œåˆ é™¤è¾“å‡ºæ–‡ä»¶å¤¹åŠå…¶å†…å®¹
    os.makedirs(path)  # åˆ›å»ºæ–°çš„è¾“å‡ºæ–‡ä»¶å¤¹


def flatten_recursive(path='../datasets/coco128'):
    # å°†é€’å½’ç›®å½•å±•å¹³ï¼Œå°†æ‰€æœ‰æ–‡ä»¶ç§»åˆ°é¡¶å±‚ç›®å½•
    # å‚æ•°:
    # path (str): è¦å±•å¹³çš„ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º '../datasets/coco128'

    new_path = Path(path + '_flat')  # åˆ›å»ºæ–°çš„è·¯å¾„ï¼Œç”¨äºå­˜æ”¾å±•å¹³åçš„æ–‡ä»¶
    create_folder(new_path)  # è°ƒç”¨ create_folder å‡½æ•°åˆ›å»ºæ–°æ–‡ä»¶å¤¹

    # ä½¿ç”¨ tqdm è¿›åº¦æ¡éå†æŒ‡å®šè·¯å¾„ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    for file in tqdm(glob.glob(str(Path(path)) + '/**/*.*', recursive=True)):
        # å¤åˆ¶æ¯ä¸ªæ–‡ä»¶åˆ°æ–°ç›®å½•
        shutil.copyfile(file, new_path / Path(file).name)  # é€šè¿‡ Path(file).name è·å–æ–‡ä»¶å


def extract_boxes(path='../datasets/coco128'):
    # å°†æ£€æµ‹æ•°æ®é›†è½¬æ¢ä¸ºåˆ†ç±»æ•°æ®é›†ï¼Œæ¯ä¸ªç±»ä¸€ä¸ªç›®å½•
    # å‚æ•°:
    # path (str): æ•°æ®é›†çš„è·¯å¾„ï¼Œé»˜è®¤ä¸º '../datasets/coco128'

    path = Path(path)  # å°†è·¯å¾„è½¬æ¢ä¸º Path å¯¹è±¡
    shutil.rmtree(path / 'classifier') if (path / 'classifier').is_dir() else None  # åˆ é™¤å·²æœ‰çš„ 'classifier' ç›®å½•

    files = list(path.rglob('*.*'))  # é€’å½’æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
    n = len(files)  # æ–‡ä»¶æ€»æ•°

    for im_file in tqdm(files, total=n):  # éå†æ¯ä¸ªæ–‡ä»¶å¹¶æ˜¾ç¤ºè¿›åº¦æ¡
        if im_file.suffix[1:] in IMG_FORMATS:  # å¦‚æœæ˜¯å›¾åƒæ–‡ä»¶
            im = cv2.imread(str(im_file))[..., ::-1]  # è¯»å–å›¾åƒå¹¶è½¬æ¢ BGR åˆ° RGB
            h, w = im.shape[:2]  # è·å–å›¾åƒçš„é«˜åº¦å’Œå®½åº¦

            # è·å–å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶
            lb_file = Path(img2label_paths([str(im_file)])[0])
            if Path(lb_file).exists():  # æ£€æŸ¥æ ‡ç­¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                with open(lb_file, 'r') as f:
                    lb = np.array([x.split() for x in f.read().strip().splitlines()], dtype=np.float32)  # è¯»å–æ ‡ç­¾

                for j, x in enumerate(lb):  # éå†æ¯ä¸ªæ ‡ç­¾
                    c = int(x[0])  # è·å–ç±»åˆ«
                    f = (path / 'classifier') / f'{c}' / f'{path.stem}_{im_file.stem}_{j}.jpg'  # æ–°æ–‡ä»¶å

                    if not f.parent.is_dir():  # åˆ›å»ºç±»ç›®å½•
                        f.parent.mkdir(parents=True)

                    b = x[1:] * [w, h, w, h]  # è®¡ç®—è¾¹ç•Œæ¡†
                    b[2:] = b[2:] * 1.2 + 3  # æ‰©å¤§è¾¹ç•Œæ¡†
                    b = xywh2xyxy(b.reshape(-1, 4)).ravel().astype(np.int)  # å°†æ¡†ä» xywh è½¬æ¢ä¸º xyxy æ ¼å¼

                    b[[0, 2]] = np.clip(b[[0, 2]], 0, w)  # é™åˆ¶æ¡†çš„ x åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                    b[[1, 3]] = np.clip(b[[1, 3]], 0, h)  # é™åˆ¶æ¡†çš„ y åæ ‡åœ¨å›¾åƒèŒƒå›´å†…

                    # ä¿å­˜å‰ªè£åçš„å›¾åƒï¼Œå¹¶æ£€æŸ¥å†™å…¥æ˜¯å¦æˆåŠŸ
                    assert cv2.imwrite(str(f), im[b[1]:b[3], b[0]:b[2]]), f'box failure in {f}'


def autosplit(path='../datasets/coco128/images', weights=(0.9, 0.1, 0.0), annotated_only=False):
    """
    è‡ªåŠ¨å°†æ•°æ®é›†åˆ†å‰²ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼Œå¹¶ä¿å­˜è·¯å¾„ä¸‹çš„ autosplit_*.txt æ–‡ä»¶
    ä½¿ç”¨æ–¹æ³•: from utils.datasets import *; autosplit()

    å‚æ•°:
        path:            å›¾åƒç›®å½•çš„è·¯å¾„
        weights:         è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•çš„æƒé‡ (åˆ—è¡¨æˆ–å…ƒç»„)
        annotated_only:  ä»…ä½¿ç”¨å¸¦æ³¨é‡Šçš„å›¾åƒ
    """
    path = Path(path)  # å°†è·¯å¾„è½¬æ¢ä¸º Path å¯¹è±¡
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    files = sum([list(path.rglob(f"*.{img_ext}")) for img_ext in IMG_FORMATS], [])  # ä»…å›¾åƒæ–‡ä»¶
    n = len(files)  # æ–‡ä»¶æ€»æ•°
    random.seed(0)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    # æ ¹æ®æƒé‡éšæœºåˆ†é…æ¯ä¸ªå›¾åƒåˆ°è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†
    indices = random.choices([0, 1, 2], weights=weights, k=n)

    # å®šä¹‰ä¸‰ä¸ª txt æ–‡ä»¶å
    txt = ['autosplit_train.txt', 'autosplit_val.txt', 'autosplit_test.txt']
    # åˆ é™¤å·²æœ‰çš„ txt æ–‡ä»¶
    [(path.parent / x).unlink(missing_ok=True) for x in txt]

    # æ‰“å°åˆ†å‰²ä¿¡æ¯
    print(f'Autosplitting images from {path}' + ', using *.txt labeled images only' * annotated_only)

    # éå†æ¯ä¸ªå›¾åƒå’Œå…¶å¯¹åº”çš„ç´¢å¼•
    for i, img in tqdm(zip(indices, files), total=n):
        # å¦‚æœåªä½¿ç”¨å¸¦æ³¨é‡Šçš„å›¾åƒï¼Œæ£€æŸ¥å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():
            # å°†å›¾åƒè·¯å¾„å†™å…¥å¯¹åº”çš„ txt æ–‡ä»¶
            with open(path.parent / txt[i], 'a') as f:
                f.write('./' + img.relative_to(path.parent).as_posix() + '\n')  # æ·»åŠ å›¾åƒåˆ° txt æ–‡ä»¶


def verify_image_label(args):
    # éªŒè¯ä¸€å¯¹å›¾åƒå’Œæ ‡ç­¾
    im_file, lb_file, prefix = args  # è§£åŒ…å‚æ•°
    nm, nf, ne, nc, msg, segments = 0, 0, 0, 0, '', []  # è®¡æ•°ï¼ˆç¼ºå¤±ã€æ‰¾åˆ°ã€ç©ºã€æŸåï¼‰ã€æ¶ˆæ¯ã€æ®µè½
    try:
        # éªŒè¯å›¾åƒ
        im = Image.open(im_file)  # æ‰“å¼€å›¾åƒæ–‡ä»¶
        im.verify()  # ä½¿ç”¨ PIL éªŒè¯å›¾åƒå®Œæ•´æ€§
        shape = exif_size(im)  # è·å–å›¾åƒå¤§å°
        # ç¡®ä¿å›¾åƒå¤§å°å¤§äº 10 åƒç´ 
        assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'
        # ç¡®ä¿å›¾åƒæ ¼å¼æœ‰æ•ˆ
        assert im.format.lower() in IMG_FORMATS, f'invalid image format {im.format}'
        if im.format.lower() in ('jpg', 'jpeg'):
            # æ£€æŸ¥ JPEG æ–‡ä»¶æ˜¯å¦æŸå
            with open(im_file, 'rb') as f:
                f.seek(-2, 2)  # å®šä½åˆ°æ–‡ä»¶å°¾éƒ¨å€’æ•°ç¬¬äºŒä¸ªå­—èŠ‚
                if f.read() != b'\xff\xd9':  # æ£€æŸ¥ JPEG æ–‡ä»¶å°¾éƒ¨
                    # é‡æ–°ä¿å­˜å›¾åƒä»¥ä¿®å¤æŸå
                    Image.open(im_file).save(im_file, format='JPEG', subsampling=0, quality=100)
                    msg = f'{prefix}WARNING: corrupt JPEG restored and saved {im_file}'

        # éªŒè¯æ ‡ç­¾
        if os.path.isfile(lb_file):
            nf = 1  # æ ‡ç­¾æ–‡ä»¶å­˜åœ¨
            with open(lb_file, 'r') as f:
                l = [x.split() for x in f.read().strip().splitlines() if len(x)]  # è¯»å–æ ‡ç­¾
                if any([len(x) > 8 for x in l]):  # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†æ®µ
                    classes = np.array([x[0] for x in l], dtype=np.float32)  # ç±»åˆ«
                    segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in l]  # (cls, xy1...)
                    # ç»„åˆç±»åˆ«å’Œè¾¹ç•Œæ¡†
                    l = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (cls, xywh)
                l = np.array(l, dtype=np.float32)  # è½¬æ¢ä¸ºæµ®ç‚¹å‹æ•°ç»„
            if len(l):
                # éªŒè¯æ ‡ç­¾çš„æ ¼å¼å’Œæœ‰æ•ˆæ€§
                assert l.shape[1] == 5, 'labels require 5 columns each'  # æ¯ä¸ªæ ‡ç­¾å¿…é¡»æœ‰ 5 åˆ—
                assert (l >= 0).all(), 'negative labels'  # ç¡®ä¿æ²¡æœ‰è´Ÿå€¼æ ‡ç­¾
                assert (l[:, 1:] <= 1).all(), 'non-normalized or out of bounds coordinate labels'  # ç¡®ä¿åæ ‡å½’ä¸€åŒ–
                assert np.unique(l, axis=0).shape[0] == l.shape[0], 'duplicate labels'  # ç¡®ä¿æ²¡æœ‰é‡å¤æ ‡ç­¾
            else:
                ne = 1  # æ ‡ç­¾ä¸ºç©º
                l = np.zeros((0, 5), dtype=np.float32)  # è¿”å›ç©ºæ ‡ç­¾
        else:
            nm = 1  # æ ‡ç­¾ç¼ºå¤±
            l = np.zeros((0, 5), dtype=np.float32)  # è¿”å›ç©ºæ ‡ç­¾
        return im_file, l, shape, segments, nm, nf, ne, nc, msg  # è¿”å›ç»“æœ
    except Exception as e:
        nc = 1  # è®¾ç½®æŸåè®¡æ•°
        msg = f'{prefix}WARNING: Ignoring corrupted image and/or label {im_file}: {e}'  # é”™è¯¯æ¶ˆæ¯
        return [None, None, None, None, nm, nf, ne, nc, msg]  # è¿”å›é”™è¯¯ç»“æœ


def dataset_stats(path='coco128.yaml', autodownload=False, verbose=False, profile=False, hub=False):
    """ è¿”å›æ•°æ®é›†ç»Ÿè®¡å­—å…¸ï¼ŒåŒ…æ‹¬æ¯ä¸ªåˆ†å‰²çš„å›¾åƒå’Œå®ä¾‹è®¡æ•°
    ç”¨æ³•1: from utils.datasets import *; dataset_stats('coco128.yaml', autodownload=True)
    ç”¨æ³•2: from utils.datasets import *; dataset_stats('../datasets/coco128_with_yaml.zip')
    å‚æ•°
        path:           data.yaml æˆ–åŒ…å« data.yaml çš„ data.zip çš„è·¯å¾„
        autodownload:   å¦‚æœæœªåœ¨æœ¬åœ°æ‰¾åˆ°æ•°æ®é›†ï¼Œåˆ™å°è¯•ä¸‹è½½
        verbose:        æ‰“å°ç»Ÿè®¡å­—å…¸
    """

    def round_labels(labels):
        # æ›´æ–°æ ‡ç­¾ä¸ºæ•´æ•°ç±»å’Œ 6 ä½å°æ•°çš„æµ®ç‚¹æ•°
        return [[int(c), *[round(x, 4) for x in points]] for c, *points in labels]

    def unzip(path):
        # è§£å‹ data.zip TODO: çº¦æŸï¼špath/to/abc.zip å¿…é¡»è§£å‹åˆ° 'path/to/abc/'
        if str(path).endswith('.zip'):  # å¦‚æœè·¯å¾„æ˜¯ data.zip
            assert Path(path).is_file(), f'Error unzipping {path}, file not found'
            ZipFile(path).extractall(path=path.parent)  # è§£å‹ç¼©
            dir = path.with_suffix('')  # æ•°æ®é›†ç›®å½• = zip åç§°
            return True, str(dir), next(dir.rglob('*.yaml'))  # è¿”å›å‹ç¼©çŠ¶æ€ã€æ•°æ®ç›®å½•å’Œ yaml è·¯å¾„
        else:  # å¦‚æœè·¯å¾„æ˜¯ data.yaml
            return False, None, path

    def hub_ops(f, max_dim=1920):
        # HUB æ“ä½œï¼Œç”¨äºè°ƒæ•´å•ä¸ªå›¾åƒ 'f' çš„å¤§å°å¹¶ä»¥é™ä½è´¨é‡ä¿å­˜åœ¨ /dataset-hub ä¸­ä»¥ä¾›ç½‘é¡µ/åº”ç”¨æŸ¥çœ‹
        f_new = im_dir / Path(f).name  # dataset-hub å›¾åƒæ–‡ä»¶å
        try:  # ä½¿ç”¨ PIL
            im = Image.open(f)
            r = max_dim / max(im.height, im.width)  # æ¯”ä¾‹
            if r < 1.0:  # å›¾åƒå¤ªå¤§
                im = im.resize((int(im.width * r), int(im.height * r)))  # è°ƒæ•´å›¾åƒå¤§å°
            im.save(f_new, quality=75)  # ä¿å­˜
        except Exception as e:  # ä½¿ç”¨ OpenCV
            print(f'WARNING: HUB ops PIL failure {f}: {e}')
            im = cv2.imread(f)
            im_height, im_width = im.shape[:2]
            r = max_dim / max(im_height, im_width)  # æ¯”ä¾‹
            if r < 1.0:  # å›¾åƒå¤ªå¤§
                im = cv2.resize(im, (int(im_width * r), int(im_height * r)), interpolation=cv2.INTER_LINEAR)  # è°ƒæ•´å›¾åƒå¤§å°
            cv2.imwrite(str(f_new), im)  # ä¿å­˜è°ƒæ•´åçš„å›¾åƒ

    zipped, data_dir, yaml_path = unzip(Path(path))  # è§£å‹æˆ–è·å–è·¯å¾„
    with open(check_yaml(yaml_path), errors='ignore') as f:
        data = yaml.safe_load(f)  # è¯»å–æ•°æ®å­—å…¸
        if zipped:
            data['path'] = data_dir  # å¦‚æœè§£å‹ç¼©ï¼Œæ›´æ–°è·¯å¾„
    check_dataset(data, autodownload)  # æ£€æŸ¥å¹¶ä¸‹è½½æ•°æ®é›†ï¼ˆå¦‚æœç¼ºå¤±ï¼‰
    hub_dir = Path(data['path'] + ('-hub' if hub else ''))  # hub ç›®å½•
    stats = {'nc': data['nc'], 'names': data['names']}  # ç»Ÿè®¡å­—å…¸

    for split in 'train', 'val', 'test':
        if data.get(split) is None:
            stats[split] = None  # å¦‚æœæ²¡æœ‰æµ‹è¯•é›†
            continue
        x = []
        dataset = LoadImagesAndLabels(data[split])  # åŠ è½½æ•°æ®é›†
        for label in tqdm(dataset.labels, total=dataset.n, desc='Statistics'):
            x.append(np.bincount(label[:, 0].astype(int), minlength=data['nc']))  # ç»Ÿè®¡æ¯ä¸ªç±»çš„å®ä¾‹
        x = np.array(x)  # è½¬æ¢ä¸ºæ•°ç»„ï¼Œå½¢çŠ¶(128x80)
        stats[split] = {
            'instance_stats': {'total': int(x.sum()), 'per_class': x.sum(0).tolist()},  # å®ä¾‹ç»Ÿè®¡
            'image_stats': {
                'total': dataset.n,  # å›¾åƒæ€»æ•°
                'unlabelled': int(np.all(x == 0, 1).sum()),  # æœªæ ‡è®°å›¾åƒæ•°
                'per_class': (x > 0).sum(0).tolist()  # æ¯ç±»çš„æ ‡è®°å›¾åƒæ•°
            },
            'labels': [{str(Path(k).name): round_labels(v.tolist())} for k, v in zip(dataset.img_files, dataset.labels)]  # æ ‡ç­¾ä¿¡æ¯
        }

        if hub:
            im_dir = hub_dir / 'images'  # hub å›¾åƒç›®å½•
            im_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç›®å½•
            for _ in tqdm(ThreadPool(NUM_THREADS).imap(hub_ops, dataset.img_files), total=dataset.n, desc='HUB Ops'):
                pass  # æ‰§è¡Œ HUB æ“ä½œ

    # æ€§èƒ½åˆ†æ
    stats_path = hub_dir / 'stats.json'
    if profile:
        for _ in range(1):
            file = stats_path.with_suffix('.npy')
            t1 = time.time()
            np.save(file, stats)  # ä¿å­˜ä¸º npy æ ¼å¼
            t2 = time.time()
            x = np.load(file, allow_pickle=True)  # åŠ è½½ npy æ–‡ä»¶
            print(f'stats.npy times: {time.time() - t2:.3f}s read, {t2 - t1:.3f}s write')

            file = stats_path.with_suffix('.json')
            t1 = time.time()
            with open(file, 'w') as f:
                json.dump(stats, f)  # ä¿å­˜ä¸º JSON æ ¼å¼
            t2 = time.time()
            with open(file, 'r') as f:
                x = json.load(f)  # åŠ è½½ JSON æ–‡ä»¶
            print(f'stats.json times: {time.time() - t2:.3f}s read, {t2 - t1:.3f}s write')

    # ä¿å­˜ã€æ‰“å°å’Œè¿”å›
    if hub:
        print(f'Saving {stats_path.resolve()}...')
        with open(stats_path, 'w') as f:
            json.dump(stats, f)  # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    if verbose:
        print(json.dumps(stats, indent=2, sort_keys=False))  # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    return stats  # è¿”å›ç»Ÿè®¡ä¿¡æ¯

